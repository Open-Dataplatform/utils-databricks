{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/Open-Dataplatform/utils-databricks.git@v0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Importing functions from the custom utility package\n",
    "from custom_utils import dataframe, helper\n",
    "from custom_utils.dp_storage import reader, writer, initialize_config\n",
    "from custom_utils.dp_storage.validation import verify_paths_and_files\n",
    "from custom_utils.dp_storage.connector import mount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7522a797",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8041c2d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f0f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configuration and helper objects\n",
    "config = initialize_config(dbutils, helper, '<source_environment>', '<destination_environment>', '<source_container>', '<source_datasetidentifier>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4870a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify paths and files\n",
    "schema_file_path, data_file_path, file_type = verify_paths_and_files(dbutils, config, helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1422d9b",
   "metadata": {},
   "source": [
    "## Read\n",
    "Reads data from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdb501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file_path = reader.get_path_to_triggering_file(\n",
    "    config.source_folder_path,\n",
    "    config.source_filename,\n",
    "    config_for_triggered_dataset=config.source_environment\n",
    ")\n",
    "\n",
    "# To get the path to a source dataset:\n",
    "# source_dataset_path = reader.get_dataset_path(config.source_environment)\n",
    "\n",
    "# Read and parse the JSON content using schema\n",
    "schema, spark_schema = reader.json_schema_to_spark_struct(schema_file_path)\n",
    "df_raw = reader.read_json_from_binary(spark, spark_schema, data_file_path)\n",
    "\n",
    "# Rewrite the line above if your file is not JSON.\n",
    "# Examples:\n",
    "# df_raw = spark.read.option(\"delimiter\", \",\").csv(source_file_path, header=True)\n",
    "# df_raw = spark.read.parquet(source_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b159ffa9",
   "metadata": {},
   "source": [
    "## Standardize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0fa14e",
   "metadata": {},
   "source": [
    "Standardize the data here. Follow this style guide: https://github.com/palantir/pyspark-style-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e922f783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of functionality\n",
    "df = dataframe.flatten_df(df_raw, depth_level=config.depth_level, type_mapping=dataframe.get_type_mapping())\n",
    "df = dataframe.rename_columns(df, replacements={'.': '_'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc6b0b6",
   "metadata": {},
   "source": [
    "## Merge and upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77668ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_path = writer.get_destination_path(config.destination_environment)\n",
    "database_name_databricks, table_name_databricks = writer.get_databricks_table_info(config.destination_environment)\n",
    "\n",
    "# Insert the processed data\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .option(\"path\", destination_path) \\\n",
    "    .saveAsTable(f'{database_name_databricks}.{table_name_databricks}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
