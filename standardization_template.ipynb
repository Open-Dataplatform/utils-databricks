{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04cc3443",
   "metadata": {},
   "source": [
    "\n",
    "# Data Standardization and Merging Pipeline\n",
    "\n",
    "This notebook is designed to handle the data standardization, validation, and merging processes for the Triton Flow Plans dataset. It covers the following steps:\n",
    "- Initializing the configuration and environment setup.\n",
    "- Loading and validating the JSON schema and source data.\n",
    "- Flattening and transforming nested data structures.\n",
    "- Performing data quality checks and ensuring data integrity.\n",
    "- Managing Delta table creation and merging updated records.\n",
    "- Generating feedback timestamps to track data processing intervals.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9cab1d",
   "metadata": {},
   "source": [
    "\n",
    "## Widget List\n",
    "\n",
    "The following widgets are used in this notebook to allow dynamic configuration:\n",
    "- **`SourceStorageAccount`**: The source storage account for the dataset.\n",
    "- **`DestinationStorageAccount`**: The destination storage account where processed data is stored.\n",
    "- **`SourceContainer`**: The container holding the source data files.\n",
    "- **`SourceDatasetidentifier`**: The identifier used for the dataset.\n",
    "- **`DepthLevel`**: The depth level for flattening nested JSON structures.\n",
    "\n",
    "These widgets enable flexibility and can be adjusted to fit different environments or datasets.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd155ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install git+https://github.com/Open-Dataplatform/utils-databricks.git@v0.6.1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fa9326",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Importing modules from custom_utils\n",
    "from custom_utils import dataframe, helper\n",
    "\n",
    "# Importing modules and functions from dp_storage\n",
    "from custom_utils.dp_storage import quality, table_management, merge_management, feedback_management\n",
    "from custom_utils.dp_storage.validation import PathValidator\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de91d13",
   "metadata": {},
   "source": [
    "\n",
    "## Setup\n",
    "### Configuration Handling with `initialize_config`\n",
    "The `initialize_config` function manages all configuration parameters dynamically. It pulls values from widgets and ADF, ensuring the code remains adaptable across different environments. This setup ensures that paths, environment settings, and data identifiers are correctly configured.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142c7f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize configuration and helper objects\n",
    "spark, config = initialize_notebook(dbutils=dbutils, helper=helper)\n",
    "config.unpack(globals())\n",
    "config.print_params()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff4603",
   "metadata": {},
   "source": [
    "\n",
    "## Read Data\n",
    "This section validates the paths, reads the JSON schema, and loads the source data for further processing. The functions here ensure that the schema is correctly converted to a PySpark `StructType`, and the data is flattened as needed.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f21498",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Validate paths and files\n",
    "validator = PathValidator(config)\n",
    "schema_file_path, data_file_path, file_type = validator.verify_paths_and_files()\n",
    "\n",
    "# Read and parse the JSON content using schema\n",
    "schema_json, spark_schema = reader.json_schema_to_spark_struct(schema_file_path)\n",
    "df_raw = reader.read_json_from_binary(spark, spark_schema, data_file_path)\n",
    "display(df_raw)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cb250d",
   "metadata": {},
   "source": [
    "\n",
    "### Flattening and Schema Handling\n",
    "The depth level is critical in controlling how deeply nested structures are flattened. The schema handling logic ensures that the JSON schema is accurately represented in PySpark, allowing for validation and transformation. In cases where the schema has deep nesting, this function can limit the flattening to a specific depth, making the data easier to manage.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Flatten and standardize the DataFrame\n",
    "df, df_flattened, columns_of_interest, view_name = dataframe.process_and_flatten_json(\n",
    "    spark=spark,\n",
    "    config=config,\n",
    "    schema_file_path=schema_file_path,\n",
    "    data_file_path=data_file_path,\n",
    "    helper=helper\n",
    ")\n",
    "display(df_flattened)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b16a4",
   "metadata": {},
   "source": [
    "\n",
    "## Data Quality Checks\n",
    "This step ensures that the data meets expected quality standards. The duplicate check, for example, verifies that key columns do not have repeated values, ensuring data integrity before further processing.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc4f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform data quality checks (e.g., duplicate checks)\n",
    "quality.perform_quality_check(\n",
    "    spark=spark,\n",
    "    key_columns=key_columns,\n",
    "    view_name=view_name,\n",
    "    helper=helper\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb2964f",
   "metadata": {},
   "source": [
    "\n",
    "## Temporary View Creation and Data Transformation\n",
    "Creating a temporary view for the most recent records based on specific key columns allows you to work with the latest data version. This is particularly useful when handling time series or datasets with versioning.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901cd1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the columns used for ordering if multiple files are read\n",
    "order_by_columns = [\"input_file_name DESC\", \"EventTimestamp DESC\"]\n",
    "\n",
    "# Create a temporary view with the most recent records\n",
    "dataframe.create_temp_view_with_most_recent_records(\n",
    "    spark=spark,\n",
    "    view_name=view_name,\n",
    "    key_columns=key_columns,\n",
    "    columns_of_interest=columns_of_interest,\n",
    "    order_by_columns=order_by_columns,\n",
    "    helper=helper\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae0359",
   "metadata": {},
   "source": [
    "\n",
    "## Delta Table Management and Merging\n",
    "The following code handles the creation and merging of Delta tables. The logic ensures that if a table doesn't exist, it is created. During a merge, Delta table versions are tracked to monitor changes, providing insight into updates and modifications.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d871ed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Manage table creation if it does not exist\n",
    "table_management.manage_table_creation(\n",
    "    spark=spark,\n",
    "    destination_environment=destination_environment,\n",
    "    source_datasetidentifier=source_datasetidentifier,\n",
    "    helper=helper\n",
    ")\n",
    "\n",
    "# Manage data merge\n",
    "merge_management.manage_data_merge(\n",
    "    spark=spark,\n",
    "    destination_environment=destination_environment,\n",
    "    source_datasetidentifier=source_datasetidentifier,\n",
    "    view_name=view_name,\n",
    "    key_columns=key_columns,\n",
    "    helper=helper\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d65b1",
   "metadata": {},
   "source": [
    "\n",
    "## Feedback Timestamps\n",
    "The final step is generating feedback timestamps for tracking processing intervals. This ensures that the data pipeline is operating within the expected time windows, making it easier to audit and debug.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bd0a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate feedback timestamps\n",
    "feedback_management.generate_feedback_timestamps(\n",
    "    spark=spark,\n",
    "    view_name=view_name,\n",
    "    feedback_column=feedback_column,\n",
    "    dbutils=dbutils,\n",
    "    helper=helper\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5cb37a",
   "metadata": {},
   "source": [
    "\n",
    "## Notebook Completion\n",
    "The notebook exits with the final feedback output, signaling successful processing.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4dc48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exit the notebook with success message\n",
    "dbutils.notebook.exit(\"Notebook completed successfully.\")\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
