{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/Open-Dataplatform/utils-databricks.git@v0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Importing functions from the custom utility package\n",
    "from custom_utils import dataframe, helper\n",
    "from custom_utils.dp_storage import reader, writer, initialize_config\n",
    "from custom_utils.dp_storage.validation import verify_paths_and_files\n",
    "from custom_utils.dp_storage.connector import mount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7522a797",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8041c2d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-description",
   "metadata": {},
   "source": [
    "### Configuration Handling with `initialize_config`\n",
    "\n",
    "The `initialize_config` function is now used to set up the configuration parameters for the notebook. This function centralizes the management of configurations and parameters, making it easier to reuse and maintain across different workflows.\n",
    "\n",
    "The function is defined in the `custom_utils.dp_storage` module and is a key part of the initialization process. It allows you to pass important settings such as source and destination environments, containers, dataset identifiers, and other related options.\n",
    "\n",
    "**Default Parameters and Usage**:\n",
    "```python\n",
    "config = initialize_config(dbutils, helper, '<source_environment>', '<destination_environment>', '<source_container>', '<source_datasetidentifier>')\n",
    "```\n",
    "- `dbutils`: Databricks utility object.\n",
    "- `helper`: Helper object for logging and parameter fetching.\n",
    "- `source_environment`, `destination_environment`, `source_container`, `source_datasetidentifier`: Core configuration parameters for handling data paths.\n",
    "\n",
    "By using this function, you can manage configurations more dynamically, pulling parameters from either widgets or ADF depending on where the code is executed. This replaces the older method of manually defining configurations in the notebook itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initialize-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configuration and helper objects\n",
    "config = initialize_config(dbutils, helper, '<source_environment>', '<destination_environment>', '<source_container>', '<source_datasetidentifier>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "read-description",
   "metadata": {},
   "source": [
    "## Read\n",
    "Reads data from storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "read-details",
   "metadata": {},
   "source": [
    "### Enhanced Data Reading Workflow\n",
    "\n",
    "The `verify_paths_and_files` function now handles much of the path validation and file retrieval logic that was previously done manually. It verifies the presence of required files (e.g., schema and source files) and determines the correct paths for further processing.\n",
    "\n",
    "The function calls `reader.get_path_to_triggering_file` internally to determine the correct path to the source files. This centralizes logic for fetching paths, improving reliability and reducing code duplication.\n",
    "\n",
    "In addition, the `reader` module provides several other key functions for handling data:\n",
    "- `json_schema_to_spark_struct`: Converts a JSON schema to a PySpark `StructType`.\n",
    "- `read_json_from_binary`: Reads and parses files as binary content, allowing for the extraction of JSON data with a specified schema.\n",
    "- `get_dataset_path`: Retrieves the full path to a dataset based on a provided configuration.\n",
    "\n",
    "These functions allow for more flexible and dynamic data ingestion, adapting to different file formats and data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify paths and files\n",
    "schema_file_path, data_file_path, file_type = verify_paths_and_files(dbutils, config, helper)\n",
    "\n",
    "# Read and parse the JSON content using schema\n",
    "schema, spark_schema = reader.json_schema_to_spark_struct(schema_file_path)\n",
    "df_raw = reader.read_json_from_binary(spark, spark_schema, data_file_path)\n",
    "\n",
    "# Rewrite the line above if your file is not JSON.\n",
    "# Examples:\n",
    "# df_raw = spark.read.option(\"delimiter\", \",\").csv(source_file_path, header=True)\n",
    "# df_raw = spark.read.parquet(source_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b159ffa9",
   "metadata": {},
   "source": [
    "## Standardize the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standardize-details",
   "metadata": {},
   "source": [
    "### Flattening and Renaming Data\n",
    "\n",
    "The `flatten_df` function is now used to standardize the data structure. It recursively flattens complex nested structures (like arrays and structs) and applies type mappings for consistent data handling. The function also includes options for customizing the depth level for flattening.\n",
    "\n",
    "`flatten_df` internally handles column renaming by replacing characters like `.` with `_`, streamlining the renaming process without the need for separate calls to `rename_columns`.\n",
    "\n",
    "This approach simplifies the overall standardization process while maintaining flexibility for different data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-standardization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the DataFrame\n",
    "df = dataframe.flatten_df(df_raw, depth_level=config.depth_level, type_mapping=dataframe.get_type_mapping())\n",
    "df = dataframe.rename_columns(df, replacements={'.': '_'})  # Optional if further renaming is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "merge-description",
   "metadata": {},
   "source": [
    "## Merge and Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "merge-details",
   "metadata": {},
   "source": [
    "### Improved Delta Table Handling and Version Tracking\n",
    "\n",
    "In this step, the processed data is merged into a Delta table. The current Delta table version is retrieved before the merge, allowing for version comparison and tracking changes between updates.\n",
    "\n",
    "You can easily check the changes between table versions by comparing the old and new versions. This helps with monitoring data updates and understanding the impact of each merge operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge-upload",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine destination paths\n",
    "destination_path = writer.get_destination_path(config.destination_environment)\n",
    "database_name_databricks, table_name_databricks = writer.get_databricks_table_info(config.destination_environment)\n",
    "\n",
    "# Get the current Delta table version before the merge\n",
    "try:\n",
    "    current_version = spark.sql(f\"DESCRIBE HISTORY {database_name_databricks}.{table_name_databricks}\").select(\"version\").orderBy(F.desc(\"version\")).first()[0]\n",
    "except:\n",
    "    current_version = None  # If the table doesn't exist yet\n",
    "\n",
    "# Insert the processed data\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .option(\"path\", destination_path) \\\n",
    "    .saveAsTable(f'{database_name_databricks}.{table_name_databricks}')\n",
    "\n",
    "# Get the new Delta table version after the merge\n",
    "new_version = spark.sql(f\"DESCRIBE HISTORY {database_name_databricks}.{table_name_databricks}\").select(\"version\").orderBy(F.desc(\"version\")).first()[0]\n",
    "\n",
    "# Print version changes\n",
    "if current_version is not None:\n",
    "    print(f\"Table updated from version {current_version} to {new_version}.\")\n",
    "else:\n",
    "    print(f\"Table created at version {new_version}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
