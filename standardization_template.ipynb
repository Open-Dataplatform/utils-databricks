{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/Open-Dataplatform/utils-databricks.git@v0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Importing functions from the custom utility package\n",
    "from custom_utils import dataframe, helper\n",
    "from custom_utils.dp_storage import reader, writer, initialize_config, table_management, merge_management, feedback_management, quality\n",
    "from custom_utils.validation import verify_paths_and_files\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7522a797",
   "metadata": {},
   "source": [
    "# Standardization Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8041c2d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-description",
   "metadata": {},
   "source": [
    "### Configuration Handling with `initialize_config`\n",
    "This template initializes the configuration, sets up paths, and prepares the environment for the data processing pipeline. The `initialize_config` function handles the configuration centrally, making it easy to reuse across notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initialize-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configuration and helper objects\n",
    "config = initialize_config(dbutils, helper, '<source_environment>', '<destination_environment>', '<source_container>', '<source_datasetidentifier>')\n",
    "spark = config.spark_session\n",
    "config.unpack(globals())\n",
    "config.print_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "read-description",
   "metadata": {},
   "source": [
    "## Read\n",
    "In this section, we load the JSON schema and source data, handle nested structures, and prepare the data for standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify paths and files\n",
    "schema_file_path, data_file_path, file_type = verify_paths_and_files(dbutils, config, helper)\n",
    "\n",
    "# Read and parse the JSON content using schema\n",
    "schema_json, spark_schema = reader.json_schema_to_spark_struct(schema_file_path)\n",
    "df_raw = reader.read_json_from_binary(spark, spark_schema, data_file_path)\n",
    "display(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b159ffa9",
   "metadata": {},
   "source": [
    "## Data Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standardize-details",
   "metadata": {},
   "source": [
    "### Flattening and Renaming Data\n",
    "We flatten complex nested structures (like arrays and structs) using the `flatten_df` function, which also applies type mappings and handles column renaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-standardization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten and standardize the DataFrame\n",
    "df, df_flattened, columns_of_interest, view_name = dataframe.process_and_flatten_json(\n",
    "    spark=spark,\n",
    "    config=config,\n",
    "    schema_file_path=schema_file_path,\n",
    "    data_file_path=data_file_path,\n",
    "    helper=helper\n",
    ")\n",
    "display(df_flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "merge-description",
   "metadata": {},
   "source": [
    "## Merge and Upload\n",
    "In this step, we manage table creation and data merging using Delta Lake. The logic tracks Delta table versions to monitor changes during the merge operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge-upload",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manage table creation if it does not exist\n",
    "table_management.manage_table_creation(\n",
    "    spark=spark,\n",
    "    destination_environment=destination_environment,\n",
    "    source_datasetidentifier=source_datasetidentifier,\n",
    "    helper=helper\n",
    ")\n",
    "\n",
    "# Manage data merge\n",
    "merge_management.manage_data_merge(\n",
    "    spark=spark,\n",
    "    destination_environment=destination_environment,\n",
    "    source_datasetidentifier=source_datasetidentifier,\n",
    "    view_name=view_name,\n",
    "    key_columns=key_columns,\n",
    "    helper=helper\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feedback-description",
   "metadata": {},
   "source": [
    "## Feedback Timestamps\n",
    "The final step is to generate feedback timestamps for tracking data processing intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feedback-timestamps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate feedback timestamps\n",
    "feedback_management.generate_feedback_timestamps(\n",
    "    spark=spark,\n",
    "    view_name=view_name,\n",
    "    feedback_column=feedback_column,\n",
    "    dbutils=dbutils,\n",
    "    helper=helper\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exit-notebook",
   "metadata": {},
   "source": [
    "## Notebook Completion\n",
    "The notebook exits after processing and logging the feedback results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "notebook-exit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exit the notebook with success message\n",
    "dbutils.notebook.exit(\"Notebook completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
