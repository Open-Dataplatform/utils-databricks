{
    "metadata": {
        "language_info": {
            "name": "python"
        },
        "application/vnd.databricks.v1+notebook": {
            "computePreferences": null,
            "dashboards": [],
            "environmentMetadata": null,
            "language": "python",
            "notebookMetadata": {
                "pythonIndentUnit": 4
            },
            "notebookName": "s-data_group__entityname",
            "widgets": {
                "DepthLevel": {
                    "currentValue": "1",
                    "nuid": "52b3453c-7ec1-48d7-aaac-9269bfc4457a",
                    "typedWidgetInfo": {
                        "autoCreated": false,
                        "defaultValue": "1",
                        "label": "Depth Level",
                        "name": "DepthLevel",
                        "options": {
                            "validationRegex": null,
                            "widgetDisplayType": "Text"
                        },
                        "parameterDataType": "String"
                    },
                    "widgetInfo": {
                        "defaultValue": "1",
                        "label": "Depth Level",
                        "name": "DepthLevel",
                        "options": {
                            "autoCreated": null,
                            "validationRegex": null,
                            "widgetType": "text"
                        },
                        "widgetType": "text"
                    }
                },
                "DestinationStorageAccount": {
                    "currentValue": "dpuniformstoragetest",
                    "nuid": "ec856836-be56-4d10-b3dd-0939583953c3",
                    "typedWidgetInfo": {
                        "autoCreated": false,
                        "defaultValue": "dpuniformstoragetest",
                        "label": "Destination Storage Account",
                        "name": "DestinationStorageAccount",
                        "options": {
                            "validationRegex": null,
                            "widgetDisplayType": "Text"
                        },
                        "parameterDataType": "String"
                    },
                    "widgetInfo": {
                        "defaultValue": "dpuniformstoragetest",
                        "label": "Destination Storage Account",
                        "name": "DestinationStorageAccount",
                        "options": {
                            "autoCreated": null,
                            "validationRegex": null,
                            "widgetType": "text"
                        },
                        "widgetType": "text"
                    }
                },
                "FeedbackColumn": {
                    "currentValue": "EventTimestamp",
                    "nuid": "77df1bbb-6823-4ab5-b61c-4aeca6da83c3",
                    "typedWidgetInfo": {
                        "autoCreated": false,
                        "defaultValue": "EventTimestamp",
                        "label": "Feedback Column",
                        "name": "FeedbackColumn",
                        "options": {
                            "validationRegex": null,
                            "widgetDisplayType": "Text"
                        },
                        "parameterDataType": "String"
                    },
                    "widgetInfo": {
                        "defaultValue": "EventTimestamp",
                        "label": "Feedback Column",
                        "name": "FeedbackColumn",
                        "options": {
                            "autoCreated": null,
                            "validationRegex": null,
                            "widgetType": "text"
                        },
                        "widgetType": "text"
                    }
                },
                "FileType": {
                    "currentValue": "json",
                    "nuid": "9159f62c-32ce-44dd-984d-8ae73a1a8252",
                    "typedWidgetInfo": {
                        "autoCreated": false,
                        "defaultValue": "json",
                        "label": "File Type",
                        "name": "FileType",
                        "options": {
                            "choices": [
                                "json",
                                "xlsx",
                                "xml"
                            ],
                            "fixedDomain": true,
                            "multiselect": false,
                            "widgetDisplayType": "Dropdown"
                        },
                        "parameterDataType": "String"
                    },
                    "widgetInfo": {
                        "defaultValue": "json",
                        "label": "File Type",
                        "name": "FileType",
                        "options": {
                            "autoCreated": null,
                            "choices": [
                                "json",
                                "xlsx",
                                "xml"
                            ],
                            "widgetType": "dropdown"
                        },
                        "widgetType": "dropdown"
                    }
                },
                "KeyColumns": {
                    "currentValue": "Guid",
                    "nuid": "0f9b36a0-8561-4672-b27b-77e8c900f4da",
                    "typedWidgetInfo": {
                        "autoCreated": false,
                        "defaultValue": "Guid",
                        "label": "KeyColumns",
                        "name": "KeyColumns",
                        "options": {
                            "validationRegex": null,
                            "widgetDisplayType": "Text"
                        },
                        "parameterDataType": "String"
                    },
                    "widgetInfo": {
                        "defaultValue": "Guid",
                        "label": "KeyColumns",
                        "name": "KeyColumns",
                        "options": {
                            "autoCreated": null,
                            "validationRegex": null,
                            "widgetType": "text"
                        },
                        "widgetType": "text"
                    }
                },
                "SchemaFolderName": {
                    "currentValue": "schemachecks",
                    "nuid": "7a0df1a7-5e42-445f-aff8-31409e06fc9d",
                    "typedWidgetInfo": {
                        "autoCreated": false,
                        "defaultValue": "schemachecks",
                        "label": "Schema Folder Name",
                        "name": "SchemaFolderName",
                        "options": {
                            "validationRegex": null,
                            "widgetDisplayType": "Text"
                        },
                        "parameterDataType": "String"
                    },
                    "widgetInfo": {
                        "defaultValue": "schemachecks",
                        "label": "Schema Folder Name",
                        "name": "SchemaFolderName",
                        "options": {
                            "autoCreated": null,
                            "validationRegex": null,
                            "widgetType": "text"
                        },
                        "widgetType": "text"
                    }
                },
                "SourceContainer": {
                    "currentValue": "landing",
                    "nuid": "c373f80c-d3b2-4767-aae2-a913d2371f9b",
                    "typedWidgetInfo": {
                        "autoCreated": false,
                        "defaultValue": "landing",
                        "label": "Source Container",
                        "name": "SourceContainer",
                        "options": {
                            "validationRegex": null,
                            "widgetDisplayType": "Text"
                        },
                        "parameterDataType": "String"
                    },
                    "widgetInfo": {
                        "defaultValue": "landing",
                        "label": "Source Container",
                        "name": "SourceContainer",
                        "options": {
                            "autoCreated": null,
                            "validationRegex": null,
                            "widgetType": "text"
                        },
                        "widgetType": "text"
                    }
                },
                "SourceDatasetidentifier": {
                    "currentValue": "triton__flow_plans",
                    "nuid": "6dc13fde-3de5-4427-91cb-791e74531801",
                    "typedWidgetInfo": {
                        "autoCreated": false,
                        "defaultValue": "triton__flow_plans",
                        "label": "Select Dataset Identifier",
                        "name": "SourceDatasetidentifier",
                        "options": {
                            "choices": [
                                "triton__flow_plans",
                                "cpx_so__nomination",
                                "ddp_em__dayahead_flows_nemo",
                                "pluto_pc__units_scadamw",
                                "ddp_cm__mfrr_settlement"
                            ],
                            "fixedDomain": true,
                            "multiselect": false,
                            "widgetDisplayType": "Dropdown"
                        },
                        "parameterDataType": "String"
                    },
                    "widgetInfo": {
                        "defaultValue": "triton__flow_plans",
                        "label": "Select Dataset Identifier",
                        "name": "SourceDatasetidentifier",
                        "options": {
                            "autoCreated": null,
                            "choices": [
                                "triton__flow_plans",
                                "cpx_so__nomination",
                                "ddp_em__dayahead_flows_nemo",
                                "pluto_pc__units_scadamw",
                                "ddp_cm__mfrr_settlement"
                            ],
                            "widgetType": "dropdown"
                        },
                        "widgetType": "dropdown"
                    }
                },
                "SourceFileName": {
                    "currentValue": "triton__flow_plans-202408*",
                    "nuid": "502e3390-b70a-4fba-bc96-6080774205cb",
                    "typedWidgetInfo": {
                        "autoCreated": false,
                        "defaultValue": "triton__flow_plans-202408*",
                        "label": "SourceFileName",
                        "name": "SourceFileName",
                        "options": {
                            "validationRegex": null,
                            "widgetDisplayType": "Text"
                        },
                        "parameterDataType": "String"
                    },
                    "widgetInfo": {
                        "defaultValue": "triton__flow_plans-202408*",
                        "label": "SourceFileName",
                        "name": "SourceFileName",
                        "options": {
                            "autoCreated": null,
                            "validationRegex": null,
                            "widgetType": "text"
                        },
                        "widgetType": "text"
                    }
                },
                "SourceStorageAccount": {
                    "currentValue": "dplandingstoragetest",
                    "nuid": "9a271846-9649-4a5a-9206-61c5d4721f7a",
                    "typedWidgetInfo": {
                        "autoCreated": false,
                        "defaultValue": "",
                        "label": "Source Storage Account",
                        "name": "SourceStorageAccount",
                        "options": {
                            "validationRegex": null,
                            "widgetDisplayType": "Text"
                        },
                        "parameterDataType": "String"
                    },
                    "widgetInfo": {
                        "defaultValue": "",
                        "label": "Source Storage Account",
                        "name": "SourceStorageAccount",
                        "options": {
                            "autoCreated": null,
                            "validationRegex": null,
                            "widgetType": "text"
                        },
                        "widgetType": "text"
                    }
                }
            }
        },
        "extensions": {
            "azuredatastudio": {
                "version": 1,
                "views": []
            }
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "# Data Standardization and Flattening (JSON, XML, and XLSX)\n",
                "\n",
                "This notebook is responsible for standardizing and flattening JSON, XML, and XLSX files. It reads the raw data from the landing zone, applies schemas for validation where applicable, flattens nested structures using `depth_level`, and then writes the transformed data as Delta Parquet files.\n",
                "\n",
                "## Key Features:\n",
                "- **Multi-format Support:**\n",
                "  - Reads JSON, XML, and XLSX files from the landing zone.\n",
                "  - Handles nested structures and various data types and formats.\n",
                "\n",
                "- **Schema Validation:**\n",
                "  - Applies predefined schemas for data validation:\n",
                "    - JSON: Schema files must be available at `landing/schemachecks/[datasetidentifier]/[datasetidentifier]_schema.json`.\n",
                "    - XML: Validates against XSD (XML Schema Definition) files.\n",
                "    - XLSX: Schema validation is not applicable.\n",
                "\n",
                "- **Data Flattening:**\n",
                "  - Supports flattening of nested structures in JSON and XML using `depth_level` for controlling the hierarchy level to flatten.\n",
                "  - Processes XLSX files into a structured, normalized format.\n",
                "\n",
                "- **Efficient Data Storage:**\n",
                "  - Saves the processed data as Delta Parquet files for efficient storage and querying.\n",
                "\n",
                "This notebook provides a flexible and robust framework for standardizing and preparing data for downstream analytics across multiple file formats.\n",
                "\n",
                "---\n",
                "\n",
                "## ADF Pipeline Integration\n",
                "- The notebook is designed to work seamlessly with Azure Data Factory (ADF) pipelines.\n",
                "- Parameters such as `SourceDatasetidentifier`, `SourceStorageAccount`, and other configurations are dynamically passed from the ADF pipeline at runtime.\n",
                "- External parameters passed by the pipeline automatically override the default or test configurations defined in the notebook.\n",
                "\n",
                "---\n",
                "\n",
                "## Function Definitions\n",
                "- Most of the functions used in this notebook are part of a reusable library hosted on GitHub: [Open-Dataplatform/utils-databricks](https://github.com/Open-Dataplatform/utils-databricks).\n",
                "- Detailed descriptions of these functions, including their purpose and usage, can be found in the GitHub repository.\n",
                "- This allows you to reuse and extend the existing functionalities in your own workflows efficiently.\n",
                "\n",
                "---\n",
                "\n",
                "## Use Guide\n",
                "1. **Clone the Notebook:**\n",
                "   - Clone this notebook and use it as a template for your data processing workflows.\n",
                "\n",
                "2. **Customize the Code:**\n",
                "   - Remove unnecessary code or datasets that are not relevant to your use case.\n",
                "   - Add custom logic and transformations based on your specific requirements.\n",
                "\n",
                "3. **Test and Validate:**\n",
                "   - Use the predefined dataset configurations as an inspiration (`triton__flow_plans`, `cpx_so__nomination`, etc.) to test and validate your notebook logic.\n",
                "\n",
                "4. **Run in Production:**\n",
                "   - Integrate this notebook into an ADF pipeline and pass the required parameters dynamically for production workflows.\n",
                "\n",
                "This notebook serves as a reusable and customizable framework for handling multi-format data standardization and flattening tasks."
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "936f07be-db8a-4ab9-8a49-b5d16806a4f1",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "844e64c3-6a5b-463e-86b6-64a4ed85bba3"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Setup"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "b01f8a5c-7c4b-49cd-9858-9eeb0f2ab315",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "e9507738-b3c7-4918-9d96-f6274832e802"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Package Installation and Management"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "53b26632-2c7e-4b39-9265-bf63f2a96697",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "0a909fc6-dd6e-4a5d-abcf-3f0e8b2c80e3"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Setup: Package Installation and Management\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# Manage and install essential Python packages for the Databricks project.\n",
                "# Ensures compatibility by specifying exact package versions where necessary.\n",
                "# Includes support for utilities, data processing, and XML/XLSX handling.\n",
                "\n",
                "# Step 1: Optional - Remove an existing version of the custom utility package\n",
                "# Uncomment the line below if a previous version of the utility needs to be removed.\n",
                "# %pip uninstall databricks-custom-utils -y\n",
                "\n",
                "# Step 2: Install required packages\n",
                "# The command below installs:\n",
                "# - Custom Databricks utilities (specific version from GitHub repository).\n",
                "# - Libraries for SQL parsing, Excel file handling, XML processing, and syntax highlighting.\n",
                "%pip install \\\n",
                "    git+https://github.com/Open-Dataplatform/utils-databricks.git@v0.7.2 \\\n",
                "    sqlparse \\\n",
                "    openpyxl \\\n",
                "    lxml \\\n",
                "    xmlschema \\\n",
                "    pygments\n",
                "\n",
                "\"\"\"\n",
                "Package Details:\n",
                "- `utils-databricks`: Custom utilities for extended functionality in Databricks.\n",
                "- `sqlparse`: SQL query parsing and formatting library.\n",
                "- `openpyxl`: Library for handling Excel (XLSX) files.\n",
                "- `lxml`: Robust library for processing XML and HTML files.\n",
                "- `xmlschema`: Tools for XML schema validation and conversion.\n",
                "- `pygments`: Syntax highlighting for code snippets in logs or reports.\n",
                "\"\"\""
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "bc867dd2-532b-4e37-bb76-be96acf95054",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "ddee0deb-c950-473d-819c-8bf136ff856d"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Initialize Logger"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "88ef1a0d-c92f-4555-bf68-1b51a4b2a666",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "4a4631d6-fff0-44ba-96b2-37660dc6b430"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Initialize Logger\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# Set up a custom logger for detailed logging and debugging throughout the notebook.\n",
                "# The logger offers advanced features, including:\n",
                "# - Debug-level logging for in-depth insights during execution.\n",
                "# - Block-style logging for structured, readable logs.\n",
                "# - Syntax highlighting for SQL queries and Python code in logs.\n",
                "\n",
                "# Step 1: Import the Logger class from the custom utilities package\n",
                "from custom_utils.logging.logger import Logger\n",
                "\n",
                "# Step 2: Initialize the Logger instance\n",
                "# - `debug=True` enables detailed logs, useful for troubleshooting and analysis.\n",
                "logger = Logger(debug=True)\n",
                "\n",
                "# Log the initialization success\n",
                "logger.log_message(\"Logger initialized successfully.\")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "96254bc3-2ad1-497d-bbd8-0fb9caa5a8a6",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "eaaee36b-d39b-47f5-8e64-e335fabb0120"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Widget Initialization and Test Configuration"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "88d0d9a0-3f4f-492a-ad35-4942ac2c6ecb",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "a750728a-09bb-4c15-86f5-a592c8eea8c2"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Clear all existing widgets"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "2912f1cb-1bc5-4be2-9082-53d461915ced",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "650d4b17-7546-4bf3-b0e3-bee39345ed6c"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Widget Initialization and Test Configuration: Clear All Widgets\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# Ensure a clean slate for widget initialization by removing all existing widgets.\n",
                "# This step prevents duplication or conflicts with previously defined widgets.\n",
                "\n",
                "# Step 1: Remove all existing widgets\n",
                "try:\n",
                "    dbutils.widgets.removeAll()\n",
                "    logger.log_message(\"All existing widgets removed successfully.\")\n",
                "except Exception as e:\n",
                "    logger.log_error(f\"Error during widget cleanup: {str(e)}\")\n",
                "    raise RuntimeError(f\"Failed to clear widgets: {str(e)}\")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "collapsed": true,
                    "inputWidgets": {},
                    "nuid": "b8232fa6-bb6d-4e6a-9e62-8456f5895ebe",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "ceaaa00d-0286-4c28-9bfb-107919993fea"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# ============================================================== \n",
                "# Widget Keys for Dynamic Initialization\n",
                "# ==============================================================\n",
                "\n",
                "# Global list of all possible widget keys (except \"SourceDatasetidentifier\")\n",
                "widget_keys = [\n",
                "    \"FileType\",\n",
                "    \"SourceStorageAccount\",\n",
                "    \"DestinationStorageAccount\",\n",
                "    \"SourceContainer\",\n",
                "    \"SourceFileName\",\n",
                "    \"KeyColumns\",\n",
                "    \"FeedbackColumn\",\n",
                "    \"DepthLevel\",\n",
                "    \"SchemaFolderName\",\n",
                "    \"XmlRootName\",\n",
                "    \"SheetName\"\n",
                "]\n",
                "\n",
                "# ============================================================== \n",
                "# Common Widgets\n",
                "# ==============================================================\n",
                "\n",
                "def initialize_common_widgets():\n",
                "    \"\"\"\n",
                "    Initializes common widgets that are the same across all datasets.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        dbutils.widgets.text(\"SourceStorageAccount\", \"dplandingstoragetest\", \"Source Storage Account\")\n",
                "        dbutils.widgets.text(\"DestinationStorageAccount\", \"dpuniformstoragetest\", \"Destination Storage Account\")\n",
                "        dbutils.widgets.text(\"SourceContainer\", \"landing\", \"Source Container\")\n",
                "        logger.log_message(\"Common widgets initialized successfully.\")\n",
                "    except Exception as e:\n",
                "        logger.log_error(f\"Error initializing common widgets: {str(e)}\")\n",
                "        raise RuntimeError(f\"Failed to initialize common widgets: {str(e)}\")\n",
                "\n",
                "# ============================================================== \n",
                "# Clear Widgets Except Common Ones\n",
                "# ==============================================================\n",
                "\n",
                "def clear_widgets_except_common():\n",
                "    \"\"\"\n",
                "    Clears all widgets except the common widgets like SourceDatasetidentifier,\n",
                "    SourceStorageAccount, DestinationStorageAccount, and SourceContainer.\n",
                "    \"\"\"\n",
                "    common_keys = [\"SourceDatasetidentifier\", \"SourceStorageAccount\", \"DestinationStorageAccount\", \"SourceContainer\"]\n",
                "    try:\n",
                "        for key in widget_keys:\n",
                "            if key not in common_keys:\n",
                "                try:\n",
                "                    dbutils.widgets.remove(key)\n",
                "                except Exception:\n",
                "                    # Ignore errors if widget doesn't exist\n",
                "                    continue\n",
                "        logger.log_message(\"Cleared all dataset-specific widgets.\")\n",
                "    except Exception as e:\n",
                "        logger.log_error(f\"Error clearing widgets: {str(e)}\")\n",
                "        raise RuntimeError(f\"Failed to clear widgets: {str(e)}\")\n",
                "\n",
                "# ============================================================== \n",
                "# Initialize Widgets Based on Selected Dataset\n",
                "# ==============================================================\n",
                "\n",
                "def initialize_widgets(selected_dataset, external_params=None):\n",
                "    \"\"\"\n",
                "    Dynamically initializes and updates widget values based on the selected dataset.\n",
                "\n",
                "    Args:\n",
                "        selected_dataset (str): The selected dataset identifier.\n",
                "        external_params (dict, optional): External parameters to override widget values.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # Step 1: Clear dataset-specific widgets except common ones\n",
                "        clear_widgets_except_common()\n",
                "\n",
                "        # Step 2: Define dataset-specific widget configurations\n",
                "        dataset_config = {\n",
                "            \"triton__flow_plans\": {\n",
                "                \"FileType\": \"json\",\n",
                "                \"SourceFileName\": \"triton__flow_plans-202408*\",\n",
                "                \"KeyColumns\": \"Guid\",\n",
                "                \"FeedbackColumn\": \"EventTimestamp\",\n",
                "                \"DepthLevel\": \"1\",\n",
                "                \"SchemaFolderName\": \"schemachecks\"\n",
                "            },\n",
                "            \"cpx_so__nomination\": {\n",
                "                \"FileType\": \"json\",\n",
                "                \"SourceFileName\": \"cpx_so__nomination-20241127T21*\",\n",
                "                \"KeyColumns\": \"flows_accountInternal_code, flows_accountExternal_code, flows_location_code, flows_direction, flows_periods_validityPeriod_begin, flows_periods_validityPeriod_end\",\n",
                "                \"FeedbackColumn\": \"dateCreated\",\n",
                "                \"DepthLevel\": \"\",\n",
                "                \"SchemaFolderName\": \"schemachecks\"\n",
                "            },\n",
                "            \"ddp_em__dayahead_flows_nemo\": {\n",
                "                \"FileType\": \"xml\",\n",
                "                \"SourceFileName\": \"ddp_em__dayahead_flows_nemo-202405*\",\n",
                "                \"KeyColumns\": \"TimeSeries_mRID, TimeSeries_Period_timeInterval_start, TimeSeries_Period_Point_position\",\n",
                "                \"FeedbackColumn\": \"timeseries_timestamp\",\n",
                "                \"DepthLevel\": \"1\",\n",
                "                \"SchemaFolderName\": \"schemachecks\",\n",
                "                \"XmlRootName\": \"Schedule_MarketDocument\"\n",
                "            },\n",
                "            \"ddp_cm__mfrr_settlement\": {\n",
                "                \"FileType\": \"xml\",\n",
                "                \"SourceFileName\": \"*\",\n",
                "                \"KeyColumns\": \"mRID, TimeSeries_mRID, TimeSeries_Period_timeInterval_start, TimeSeries_Period_Point_position, TimeSeries_Period_resolution\",\n",
                "                \"FeedbackColumn\": \"input_file_name\",\n",
                "                \"DepthLevel\": \"\",\n",
                "                \"SchemaFolderName\": \"schemachecks\",\n",
                "                \"XmlRootName\": \"ReserveAllocationResult_MarketDocument\"\n",
                "            },\n",
                "            \"pluto_pc__units_scadamw\": {\n",
                "                \"FileType\": \"xlsx\",\n",
                "                \"SourceFileName\": \"UnitsSCADAMW.xlsx\",\n",
                "                \"KeyColumns\": \"Unit_GSRN\",\n",
                "                \"SheetName\": \"Sheet\"\n",
                "            }\n",
                "        }\n",
                "\n",
                "        # Step 3: Validate selected dataset and get its configuration\n",
                "        if selected_dataset not in dataset_config:\n",
                "            raise ValueError(f\"Unknown dataset identifier: {selected_dataset}\")\n",
                "        dataset_widgets = dataset_config[selected_dataset]\n",
                "\n",
                "        # Step 4: Create widgets dynamically based on dataset configuration\n",
                "        for key, value in dataset_widgets.items():\n",
                "            dbutils.widgets.text(key, value, key)\n",
                "\n",
                "        # Step 5: Apply external parameters if provided\n",
                "        if external_params:\n",
                "            for key, value in external_params.items():\n",
                "                if key in dataset_widgets:\n",
                "                    dbutils.widgets.text(key, value, key)\n",
                "\n",
                "        logger.log_message(f\"Widgets initialized and updated for dataset: {selected_dataset}\")\n",
                "\n",
                "    except Exception as e:\n",
                "        logger.log_error(f\"Error initializing widgets for dataset {selected_dataset}: {str(e)}\")\n",
                "        raise RuntimeError(f\"Failed to initialize widgets for dataset: {selected_dataset}\")\n",
                "\n",
                "\n",
                "# ============================================================== \n",
                "# Main Execution: Initialize SourceDatasetidentifier Dropdown\n",
                "# ==============================================================\n",
                "\n",
                "try:\n",
                "    # Step 1: Initialize common widgets\n",
                "    initialize_common_widgets()\n",
                "\n",
                "    # Step 2: Ensure SourceDatasetidentifier exists\n",
                "    try:\n",
                "        dbutils.widgets.get(\"SourceDatasetidentifier\")\n",
                "    except Exception:\n",
                "        # Initialize SourceDatasetidentifier dropdown if it doesn't exist\n",
                "        dbutils.widgets.dropdown(\n",
                "            \"SourceDatasetidentifier\",\n",
                "            \"triton__flow_plans\",  # Default value\n",
                "            [\n",
                "                \"triton__flow_plans\",\n",
                "                \"cpx_so__nomination\",\n",
                "                \"ddp_em__dayahead_flows_nemo\",\n",
                "                \"pluto_pc__units_scadamw\",\n",
                "                \"ddp_cm__mfrr_settlement\"\n",
                "            ],\n",
                "            \"Select Dataset Identifier\"\n",
                "        )\n",
                "        logger.log_message(\"SourceDatasetidentifier dropdown initialized successfully.\")\n",
                "\n",
                "    # Step 3: Get the current value of SourceDatasetidentifier\n",
                "    selected_dataset = dbutils.widgets.get(\"SourceDatasetidentifier\")\n",
                "\n",
                "    # Step 4: Initialize widgets for the selected dataset\n",
                "    initialize_widgets(selected_dataset)\n",
                "\n",
                "except Exception as e:\n",
                "    logger.log_error(f\"Error during widget initialization: {str(e)}\")\n",
                "    raise RuntimeError(f\"Widget initialization failed: {str(e)}\")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "collapsed": true,
                    "inputWidgets": {},
                    "nuid": "cf7fdcd5-7f9b-4779-8b97-2c9c73dbe906",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "b9d780f4-4a4d-4bd5-a3ae-a9d145f4c822"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Initialize Notebook and Retrieve Parameters"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "c176ea42-b7e2-4452-a863-6b21856c7d58",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "5ee30e8d-878e-47d7-8ea0-5a1c7993d036"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Initialize Notebook and Retrieve Parameters\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# Set up the notebook by initializing its configuration and retrieving essential parameters.\n",
                "# This ensures centralized management of settings and enables efficient debugging\n",
                "# through a consistent configuration framework.\n",
                "\n",
                "# Step 1: Import the Config class from the custom utilities package\n",
                "from custom_utils.config.config import Config\n",
                "\n",
                "# Step 2: Initialize the Config object\n",
                "# - Pass `dbutils` for accessing Databricks workspace resources.\n",
                "# - Set `debug=False` to disable verbose debug logs for cleaner execution.\n",
                "config = Config.initialize(dbutils=dbutils, debug=False)\n",
                "\n",
                "# Step 3: Unpack configuration parameters\n",
                "# - Extracts configuration values into the notebook's global scope.\n",
                "# - This simplifies access to parameters by making them available as standard variables.\n",
                "config.unpack(globals())"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "d66fa72a-2159-4252-b4df-b7c68c668fe5",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "c3a9c9bd-e4a3-4bf9-a056-23ba455c2ba9"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Verify paths and files"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "d3f7c883-d519-44cb-ba64-b4c6ea08eb75",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "b2161721-ed76-447f-b180-6584ac274a12"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Verify Paths and Files\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# Validate the required paths and files to ensure all necessary resources \n",
                "# are available for processing. This pre-check prevents runtime errors \n",
                "# by identifying and addressing issues early in the notebook execution.\n",
                "\n",
                "# Step 1: Import the Validator class from the custom utilities package\n",
                "from custom_utils.validation.validation import Validator\n",
                "\n",
                "# Step 2: Initialize the Validator\n",
                "# - Pass `config` to access path and file parameters from the configuration.\n",
                "# - Set `debug=False` for standard validation logging without verbose output.\n",
                "validator = Validator(config=config, debug=False)\n",
                "\n",
                "# Step 3: Unpack validation parameters\n",
                "# - Extracts validation-related parameters into the notebook's global scope.\n",
                "validator.unpack(globals())\n",
                "\n",
                "# Step 4: Perform validation and check for an exit flag\n",
                "# - If critical validation fails, the notebook execution is terminated.\n",
                "validator.check_and_exit()\n"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "f09f5cf1-ba07-4fcc-9408-16dcb9aa0095",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "a5ad6661-e785-4a1a-8f11-e25ff7620d85"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Exit the Notebook if Validation Fails"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "ea7667b5-ff45-402f-b89a-fa071c62c090",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "68aed67d-53e4-4b3a-9657-9b889733e0bc"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Exit the Notebook if Validation Fails\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# Stop notebook execution gracefully if critical validation checks fail.\n",
                "# If validation passes, continue processing with a confirmation message.\n",
                "\n",
                "# Step 1: Check for an exit condition flagged by the Validator\n",
                "if Validator.exit_notebook:\n",
                "    # Step 2: Log the exit message using the logger\n",
                "    # - Provides context on why the notebook execution is being terminated.\n",
                "    logger.log_error(Validator.exit_notebook_message, level=\"error\")\n",
                "    \n",
                "    # Step 3: Exit the notebook with a descriptive message\n",
                "    # - Uses Databricks utilities to terminate execution cleanly.\n",
                "    dbutils.notebook.exit(f\"Notebook exited: {Validator.exit_notebook_message}\")\n",
                "else:\n",
                "    # Step 4: Log a success message if validation passed\n",
                "    # - Confirms the notebook will continue execution.\n",
                "    logger.log_message(\"Validation passed. The notebook is proceeding without exiting.\", level=\"info\")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "9245ce8b-8064-4392-961e-87d13a3dbea4",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "adad47b0-9433-46d9-aa93-9f255ccdda7d"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Processing Workflow"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "ce29e7ed-b443-4187-95cc-40020ece5088",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "58ec6412-b3db-4f17-830f-47dd523d1874"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Flattening and Processing"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "20671e6a-d5d5-4d87-95fb-3097241c2c4c",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "d0427549-925b-44a9-8d35-c0cdfd6a7e85"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Processing Workflow - Flattening and Processing\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# This section executes the core data processing workflow, which includes:\n",
                "# - Flattening complex hierarchical data for simplified querying and analysis.\n",
                "# - Applying optional dataset-specific transformations to align with business requirements.\n",
                "\n",
                "from pyspark.sql.functions import col\n",
                "from custom_utils.transformations.dataframe import DataFrameTransformer\n",
                "\n",
                "# Initialize the DataFrameTransformer\n",
                "# - Uses the current configuration and disables debug mode for standard operation.\n",
                "transformer = DataFrameTransformer(config=config, debug=False)\n",
                "\n",
                "try:\n",
                "    # Step 1: Process and flatten the data\n",
                "    # - Produces both the initial DataFrame and its flattened version.\n",
                "    # - depth_level controls the level of flattening for nested structures.\n",
                "    df_initial, df_flattened = transformer.process_and_flatten_data(depth_level=depth_level)\n",
                "\n",
                "    # Step 2: Apply dataset-specific transformations (if applicable)\n",
                "    # The following examples demonstrate how to implement renaming and casting\n",
                "    # for specific datasets. These transformations are optional and can be removed\n",
                "    # or customized based on dataset requirements.\n",
                "\n",
                "    # Example: Triton flow plans\n",
                "    # - Renames the column \"Timestamp\" to \"EventTimestamp\".\n",
                "    # - Casts the \"Timestamp\" field to the timestamp data type for consistency.\n",
                "    if config.source_datasetidentifier == \"triton__flow_plans\":\n",
                "        df_flattened = (\n",
                "            df_flattened\n",
                "            .withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
                "            .withColumnRenamed(\"Timestamp\", \"EventTimestamp\")\n",
                "        )\n",
                "        logger.log_message(\"Applied transformations for 'triton__flow_plans'.\", level=\"info\")\n",
                "\n",
                "    # Example: CPX SO nomination\n",
                "    # - Casts specific fields to timestamp for consistent temporal representation.\n",
                "    # - This is necessary for fields like validity periods and timestamps.\n",
                "    if config.source_datasetidentifier == \"cpx_so__nomination\":\n",
                "        df_flattened = (\n",
                "            df_flattened\n",
                "            .withColumn(\"dateCreated\", col(\"dateCreated\").cast(\"timestamp\"))\n",
                "            .withColumn(\"validityPeriod_begin\", col(\"validityPeriod_begin\").cast(\"timestamp\"))\n",
                "            .withColumn(\"validityPeriod_end\", col(\"validityPeriod_end\").cast(\"timestamp\"))\n",
                "            .withColumn(\"flows_periods_validityPeriod_begin\", col(\"flows_periods_validityPeriod_begin\").cast(\"timestamp\"))\n",
                "            .withColumn(\"flows_periods_validityPeriod_end\", col(\"flows_periods_validityPeriod_end\").cast(\"timestamp\"))\n",
                "        )\n",
                "        logger.log_message(\"Applied transformations for 'cpx_so__nomination'.\", level=\"info\")\n",
                "\n",
                "    # Step 3: Display the initial and flattened DataFrames for user verification\n",
                "    # - Provides a visual check for the raw and processed data.\n",
                "    logger.log_block(\"Displaying the initial and flattened DataFrames.\", level=\"info\")\n",
                "    logger.log_message(\"Initial DataFrame:\", level=\"info\")\n",
                "    display(df_initial)\n",
                "\n",
                "    logger.log_message(\"Flattened DataFrame:\", level=\"info\")\n",
                "    display(df_flattened)\n",
                "\n",
                "except Exception as e:\n",
                "    # Step 4: Handle errors gracefully\n",
                "    # - Logs the error details for debugging and terminates the process.\n",
                "    logger.log_message(f\"Error during processing: {str(e)}\", level=\"error\")\n",
                "    dbutils.notebook.exit(f\"Processing failed: {str(e)}\")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "baa8fc55-134f-4ba1-8290-07a3eea5fbc4",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "dd7c8158-7ecf-4a26-b090-6f9a3cf49956"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Quality check "
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "085b86e5-b48d-498b-8a05-6416832c293a",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "ee020d27-3668-4ad8-b827-7f7b68e92c4b"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Perform Quality Check and Remove Duplicates"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "f4d08f7e-8289-4b54-a81c-eaa96a0cb0be",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "fb14ad78-d5a3-4d7d-a312-35201d9e5cb3"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Quality Check - Perform Quality Check and Remove Duplicates\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# This section performs data quality checks to ensure:\n",
                "# - The integrity, accuracy, and consistency of the processed data.\n",
                "# - Duplicate records are identified and optionally removed.\n",
                "# - Additional quality checks (e.g., null value checks, value range checks) are executed.\n",
                "\n",
                "from custom_utils.quality.quality import DataQualityManager\n",
                "\n",
                "# Step 1: Initialize the DataQualityManager\n",
                "# - This class manages all quality check operations and logs relevant information.\n",
                "quality_manager = DataQualityManager(logger=logger, debug=True)\n",
                "\n",
                "# Step 2: Log available quality checks\n",
                "# - Provides an overview of checks supported by the quality manager for user reference.\n",
                "quality_manager.describe_available_checks()\n",
                "\n",
                "# Step 3: Execute data quality checks on the flattened DataFrame\n",
                "try:\n",
                "    # Perform quality checks with the following configurations:\n",
                "    cleaned_data_view = quality_manager.perform_data_quality_checks(\n",
                "        spark=spark,  # Required: Spark session.\n",
                "        df=df_flattened,  # Required: DataFrame to perform quality checks on.\n",
                "        \n",
                "        # Key columns for partitioning and duplicate checking.\n",
                "        # - Required parameter to identify unique records in the dataset.\n",
                "        key_columns=key_columns,\n",
                "        \n",
                "        # Optional: Columns for ordering within partitions (e.g., to select the latest record).\n",
                "        # - Defaults to `key_columns` if not provided.\n",
                "        order_by=feedback_column,\n",
                "        \n",
                "        # Optional: Column to use for duplicate removal ordering.\n",
                "        # - If not provided, falls back to `key_columns`.\n",
                "        feedback_column=feedback_column,\n",
                "        \n",
                "        # Optional: Column for referential integrity check against a reference DataFrame.\n",
                "        # - Ensures that foreign key relationships are maintained.\n",
                "        join_column=key_columns,\n",
                "        \n",
                "        # Optional: Exclude specified columns from the final DataFrame.\n",
                "        # - For example, `input_file_name` is excluded to avoid irrelevant data in output.\n",
                "        columns_to_exclude=[\"input_file_name\"],\n",
                "        \n",
                "        # Optional: Specify whether to use Python or SQL syntax for quality checks.\n",
                "        # - Default is SQL-based for optimized performance.\n",
                "        use_python=False\n",
                "    )\n",
                "\n",
                "    # Description of Arguments:\n",
                "    # - `spark`: Spark session used for distributed processing (required).\n",
                "    # - `df`: The DataFrame on which quality checks are performed (required).\n",
                "    # - `key_columns`: Columns used for identifying unique records (required).\n",
                "    # - `order_by`: Columns for ordering within partitions (optional; defaults to `key_columns`).\n",
                "    # - `feedback_column`: Column used for ordering duplicates (optional; falls back to `key_columns`).\n",
                "    # - `join_column`: Column for referential integrity validation (optional).\n",
                "    # - `columns_to_exclude`: List of columns to exclude from the final DataFrame (optional).\n",
                "    # - `use_python`: Boolean flag to select Python-based or SQL-based operations (optional).\n",
                "\n",
                "except Exception as e:\n",
                "    # Handle any errors during the quality check process\n",
                "    logger.log_error(f\"Error during quality check: {str(e)}\")\n",
                "    raise RuntimeError(f\"Quality check failed: {str(e)}\")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "ffec0b21-8b5f-43e1-8d4a-6ff41a9ae3e5",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "c348a776-f595-4db4-8732-c04154bf16e8"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Unified Data Management"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "eabbf4d3-4ec1-4906-b613-ca3bb7b0a30d",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "0da8aa51-0949-488d-aa8c-2eadbae70ceb"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Table Creation and Data Merging"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "ef3d2f57-0b96-4ac3-b4de-c80df0a9f996",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "8ddf76e2-35d8-4678-9830-465934ec4628"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Unified Data Management: Table Creation and Data Merging\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# This section handles the creation of destination tables and merges\n",
                "# processed data into the respective storage location. It ensures:\n",
                "# - Data is written to a unified storage with consistent formatting.\n",
                "# - Merging supports updates, inserts, and deletions seamlessly.\n",
                "# - Storage operations are managed efficiently with robust logging.\n",
                "\n",
                "from custom_utils.catalog.catalog_utils import DataStorageManager\n",
                "\n",
                "# Step 1: Initialize the DataStorageManager\n",
                "# - Manages operations related to data storage and merging.\n",
                "# - Includes detailed logging and debugging capabilities.\n",
                "storage_manager = DataStorageManager(logger=logger, debug=True)\n",
                "\n",
                "# Step 2: Perform the data storage operation\n",
                "try:\n",
                "    # Manage data operation with the following configurations (these parameters are defined in the configuration and passed as parameters to the function, and can be overridden if necessary):\n",
                "    # The used variables (cleaned_data_view, key_columns, etc.) are created and assigned by call to config.unpack(globals()) above, under section \"Initialize Notebook and Retrieve Parameters\"\n",
                "    storage_manager.manage_data_operation(\n",
                "        spark=spark,  # Required: Spark session for executing SQL or DataFrame operations.\n",
                "        dbutils=dbutils,  # Required: Databricks utilities for interacting with storage.\n",
                "\n",
                "        # Name of the cleaned data view containing the processed DataFrame.\n",
                "        # - Required parameter that holds the cleaned and transformed data.\n",
                "        cleaned_data_view=cleaned_data_view,\n",
                "\n",
                "        # Key columns used for matching records during the merge operation.\n",
                "        # - Required parameter to ensure data consistency during updates.\n",
                "        key_columns=key_columns,\n",
                "\n",
                "        # Destination folder path for storing data as Delta files.\n",
                "        # - Optional: If not provided, a default path defined in the configuration is used.\n",
                "        destination_folder_path=destination_data_folder_path,\n",
                "\n",
                "        # Target database or environment for storing the data.\n",
                "        # - Optional: Overrides the default destination environment if provided.\n",
                "        destination_environment=destination_environment,\n",
                "\n",
                "        # Target table name or identifier for the source dataset.\n",
                "        # - Optional: Allows dynamic specification of the target table for merging.\n",
                "        source_datasetidentifier=source_datasetidentifier,\n",
                "\n",
                "        # Boolean flag to select SQL-based (default) or Python DataFrame-based operations.\n",
                "        # - Optional: Default is `False` to prioritize SQL for performance.\n",
                "        use_python=False\n",
                "    )\n",
                "\n",
                "    # Description of Arguments:\n",
                "    # - `spark`: Active Spark session (required).\n",
                "    # - `dbutils`: Databricks utilities object for workspace interaction (required).\n",
                "    # - `cleaned_data_view`: Name of the view containing cleaned data (required).\n",
                "    # - `key_columns`: Columns used for identifying unique records during merge (required).\n",
                "    # - `destination_folder_path`: Override for the destination folder (optional).\n",
                "    # - `destination_environment`: Override for the target database/environment (optional).\n",
                "    # - `source_datasetidentifier`: Override for the source dataset/table identifier (optional).\n",
                "    # - `use_python`: Boolean flag for using Python or SQL operations (optional).\n",
                "\n",
                "    # Log success\n",
                "    logger.log_message(\"Data successfully written and merged into the destination table.\")\n",
                "\n",
                "except Exception as e:\n",
                "    # Step 3: Handle errors during the data storage process\n",
                "    # - Logs the error details and raises a RuntimeError to terminate execution.\n",
                "    logger.log_error(f\"Error during data storage operation: {str(e)}\")\n",
                "    raise RuntimeError(f\"Data storage operation failed: {str(e)}\")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "85023e47-eda5-4261-8b37-c10aef3c5b69",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "5e33378e-cd75-4a5d-9d61-1877291cbc0b"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Finishing"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "d6e89aa8-7435-4f5e-84b6-401f59960dfc",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "b6753c2e-222a-44d4-a6a7-60a9e31ce60f"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Return period (from_datetime, to_datetime) covered by data read"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "6b2dc900-70de-42a7-8fae-6cfb150011a6",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "6abe5114-a22c-4e9c-b148-8214a8e23049"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Finishing - Return Period Covered by Data Read\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# This section generates the feedback timestamps, providing the time \n",
                "# period covered by the processed and stored data. It calculates \n",
                "# `from_datetime` and `to_datetime` based on the data in the cleaned \n",
                "# data view.\n",
                "\n",
                "# Description of the function:\n",
                "# - `generate_feedback_timestamps`: A method to calculate and return \n",
                "#   feedback timestamps for the data processing period.\n",
                "# - Uses either the `feedback_column` or the first column in `key_columns` \n",
                "#   to determine the timestamp range.\n",
                "# \n",
                "# Parameters:\n",
                "# - `spark (SparkSession)`: Required. Active Spark session.\n",
                "# - `view_name (str)`: Required. The name of the cleaned data view.\n",
                "# - `feedback_column (Optional[str])`: Optional. Column to use for calculating \n",
                "#   feedback timestamps. Defaults to None, in which case the first column in \n",
                "#   `key_columns` is used.\n",
                "# - `key_columns (Optional[Union[str, List[str]]])`: Optional. Columns to group \n",
                "#   by when calculating the timestamp range.\n",
                "\n",
                "# Step 1: Generate feedback timestamps\n",
                "try:\n",
                "    # Call the `generate_feedback_timestamps` function with the following:\n",
                "    # - The active Spark session to access the view.\n",
                "    # - The name of the cleaned data view containing processed data.\n",
                "    # - The `feedback_column` for timestamp calculations (optional).\n",
                "    # - The `key_columns` for grouping (optional).\n",
                "    notebook_output = storage_manager.generate_feedback_timestamps(\n",
                "        spark=spark,  # Active Spark session\n",
                "        view_name=cleaned_data_view,  # The view containing cleaned and processed data\n",
                "        feedback_column=feedback_column,  # Column used for identifying feedback periods\n",
                "        key_columns=key_columns  # Key columns for grouping and extracting timestamp bounds\n",
                "    )\n",
                "\n",
                "    # Log the successful generation of feedback timestamps\n",
                "    logger.log_message(\"Feedback timestamps successfully generated.\", level=\"info\")\n",
                "\n",
                "except Exception as e:\n",
                "    # Handle errors during feedback timestamp generation\n",
                "    logger.log_error(f\"Error generating feedback timestamps: {str(e)}\")\n",
                "    raise RuntimeError(f\"Failed to generate feedback timestamps: {str(e)}\")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "85f6e198-9eb1-473a-b7b3-89d78516f86e",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "0d79f494-fd77-4335-8d03-275818e06e51"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Exit the notebook"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "6cd18333-5e15-45ca-8105-dd6823a9a3cc",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "f5312fd1-4502-47bf-b873-9b47bf2e263e"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Exit the Notebook\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# Finalize the notebook execution by exiting and returning the output.\n",
                "# The output provides a summary of the period covered by the processed data,\n",
                "# ensuring a clear handoff to any downstream workflows.\n",
                "\n",
                "# Step 1: Exit the notebook with the generated output\n",
                "try:\n",
                "    # Use dbutils to exit the notebook gracefully\n",
                "    # - The `notebook_output` contains feedback timestamps or relevant results.\n",
                "    dbutils.notebook.exit(notebook_output)\n",
                "\n",
                "    # Log the successful exit for tracking and debugging purposes\n",
                "    logger.log_message(f\"Notebook exited successfully with output: {notebook_output}\", level=\"info\")\n",
                "\n",
                "except Exception as e:\n",
                "    # Handle errors during the exit process\n",
                "    # - Logs the error and raises a RuntimeError to signal failure.\n",
                "    logger.log_error(f\"Error during notebook exit: {str(e)}\")\n",
                "    raise RuntimeError(f\"Failed to exit the notebook: {str(e)}\")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "029a6b4e-8e74-43bd-b67c-d0b7a3109808",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "3e0fb529-83dd-4a15-9afe-16df0e14a9f8"
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}