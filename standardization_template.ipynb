{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/Open-Dataplatform/utils-databricks.git@v0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "from custom_utils.dp_storage.connector import mount\n",
    "from custom_utils.dp_storage import reader, writer\n",
    "from custom_utils import adf\n",
    "from custom_utils import dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7522a797",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8041c2d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f0f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define source and destination configurations\n",
    "default_source_config = {\"<dataset_identifier>\": {\"type\":\"adls\", \"dataset\":\"<dataset_name>\", \"container\":\"<container>\", \"account\":\"<storage_account>\"}}\n",
    "default_destination_config = {\"<dataset_identifier>\": {\"type\":\"adls\", \"dataset\":\"<dataset_name>\", \"container\":\"<container>\", \"account\":\"<storage_account>\"}}\n",
    "\n",
    "# Get the configs from ADF if executed from ADF\n",
    "source_config = adf.get_source_config(dbutils, default_source_config)\n",
    "destination_config = adf.get_destination_config(dbutils, default_destination_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4870a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add or remove parameters below.\n",
    "source_folder_path = adf.get_parameter(dbutils, 'SourceFolderPath')  # Remember that it has the format \"<container>/<directory>\"\n",
    "source_filename = adf.get_parameter(dbutils, 'SourceFileName')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1422d9b",
   "metadata": {},
   "source": [
    "## Read\n",
    "Reads data from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdb501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file_path = reader.get_path_to_triggering_file(\n",
    "    source_folder_path,\n",
    "    source_filename,\n",
    "    config_for_triggered_dataset=source_config['<dataset_identifier>']\n",
    ")\n",
    "\n",
    "# To get the path to a source dataset:\n",
    "# source_dataset_path = reader.get_dataset_path(source_config['<dataset_identifier>'])\n",
    "\n",
    "df_raw = spark \\\n",
    "    .read \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .format('json').load(source_file_path)\n",
    "\n",
    "# Rewrite the line above, if your file is not JSON. \n",
    "# Examples:\n",
    "# df_raw = spark.read.option(\"delimiter\", \",\").csv(source_path, header=True)\n",
    "# df_raw = spark.read.parquet(source_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b159ffa9",
   "metadata": {},
   "source": [
    "## Standardize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0fa14e",
   "metadata": {},
   "source": [
    "Standardize the data here. Follow this style guide: https://github.com/palantir/pyspark-style-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e922f783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of functionality\n",
    "df = dataframe.flatten(df_raw, layer_separator='_')\n",
    "df = dataframe.rename_columns(df, replacements={'.': '_'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc6b0b6",
   "metadata": {},
   "source": [
    "## Merge and upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77668ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_path = writer.get_destination_path(destination_config)\n",
    "database_name_databricks, table_name_databricks = writer.get_databricks_table_info(destination_config)\n",
    "\n",
    "# Chech if the delta table exists. Else it should do a first-time-write.\n",
    "if DeltaTable.isDeltaTable(spark, destination_path):\n",
    "    dest_table = DeltaTable.forPath(spark, destination_path)\n",
    "    \n",
    "    # TODO: Specify the write pattern. Below is an example with upsert.\n",
    "    dest_table.alias(\"t\") \\\n",
    "        .merge(\n",
    "            df.alias(\"s\"),\n",
    "            \"s.some_column = t.some_column and s.another_column = t.another_column\"\n",
    "        ) \\\n",
    "        .whenMatchedUpdateAll() \\\n",
    "        .whenNotMatchedInsertAll() \\\n",
    "        .execute()\n",
    "\n",
    "else:\n",
    "    # Insert for the first time\n",
    "    df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .option(\"path\", destination_path) \\\n",
    "        .saveAsTable(f'{database_name_databricks}.{table_name_databricks}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
