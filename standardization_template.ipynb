{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Standardization and Merging Pipeline\n",
    "This notebook standardizes and merges data for the Triton Flow Plans dataset, performing the following key steps:\n",
    "- **Configuration and Environment Setup**: Initializes and manages settings.\n",
    "- **Data Validation and Loading**: Reads and verifies JSON schemas and source data.\n",
    "- **Flattening and Transforming Data**: Flattens nested JSON structures.\n",
    "- **Data Quality Checks**: Ensures data integrity by detecting duplicates.\n",
    "- **Temporary View Creation**: Takes the most recent data version if multiple files are read.\n",
    "- **Delta Table Management**: Creates and manages Delta tables, merging data when needed.\n",
    "- **Feedback Timestamp Generation**: Tracks data processing intervals for auditing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widget List\n",
    "The notebook uses the following widgets for dynamic configuration:\n",
    "- **`SourceStorageAccount`**: Source storage account name.\n",
    "- **`DestinationStorageAccount`**: Destination storage account name.\n",
    "- **`SourceContainer`**: Container storing source data.\n",
    "- **`SourceDatasetidentifier`**: Identifier for the dataset being processed.\n",
    "- **`DepthLevel`**: The depth level for flattening nested structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/Open-Dataplatform/utils-databricks.git@v0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Importing functions from the custom utility package\n",
    "from custom_utils import dataframe, helper\n",
    "from custom_utils.dp_storage import reader, writer, initialize_config, table_management, merge_management, feedback_management, quality\n",
    "from custom_utils.validation import PathValidator\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Configuration Handling with `initialize_config`\n",
    "The `initialize_config` function handles dynamic configuration management by pulling values from widgets and ADF pipelines. This setup allows the notebook to adapt to different environments seamlessly, configuring paths, environments, and dataset settings based on input parameters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configuration and helper objects\n",
    "config = initialize_config(dbutils, helper, '<source_environment>', '<destination_environment>', '<source_container>', '<source_datasetidentifier>')\n",
    "spark = config.spark_session\n",
    "config.unpack(globals())\n",
    "config.print_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Parameters\n",
    "This step retrieves critical parameters from widgets or ADF, ensuring dynamic behavior and adaptability across different environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Paths and Files\n",
    "The notebook validates and verifies paths for schema and data files. This step ensures that all required files are present before proceeding with further processing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate paths and files\n",
    "validator = PathValidator(config)\n",
    "schema_file_path, data_file_path, file_type = validator.verify_paths_and_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "This section reads the JSON schema and loads the source data. The schema is converted to PySpark's `StructType`, allowing easy validation and transformation of nested data structures."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and parse the JSON content using schema\n",
    "schema_json, spark_schema = reader.json_schema_to_spark_struct(schema_file_path)\n",
    "df_raw = reader.read_json_from_binary(spark, spark_schema, data_file_path)\n",
    "display(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening and Processing Nested JSON Data\n",
    "The `process_and_flatten_json` function orchestrates data flattening. It determines the schema depth level and applies appropriate flattening rules while handling nested structures, arrays, and objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Key Functions:\n",
    "- **`flatten_df`**: Recursively flattens nested data structures up to a specified depth.\n",
    "- **`get_json_depth`**: Determines the maximum depth of nested JSON structures, which is key for configuring flattening behavior.\n",
    "- **`rename_and_cast_columns`**: Handles renaming and casting columns to the appropriate types, avoiding conflicts with reserved SQL keywords."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten and standardize the DataFrame\n",
    "df, df_flattened, columns_of_interest, view_name = dataframe.process_and_flatten_json(\n",
    "    spark=spark,\n",
    "    config=config,\n",
    "    schema_file_path=schema_file_path,\n",
    "    data_file_path=data_file_path,\n",
    "    helper=helper\n",
    ")\n",
    "\n",
    "# Rename 'Timestamp' to 'EventTimestamp' to avoid SQL conflicts and cast it to timestamp\n",
    "if 'Timestamp' in df_flattened.columns:\n",
    "    df_flattened = dataframe.rename_and_cast_columns(\n",
    "        df_flattened,\n",
    "        column_mapping={'Timestamp': 'EventTimestamp'},\n",
    "        cast_type_mapping={'EventTimestamp': 'timestamp'},\n",
    "    )\n",
    "display(df_flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Check - Abort if Duplicates Exist in New Data\n",
    "This step ensures that no duplicate records are introduced when ingesting new data. It checks key columns for duplicates and aborts the process if any are found, ensuring data integrity."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the quality check\n",
    "quality.perform_quality_check(spark=spark, key_columns=key_columns, view_name=view_name, helper=helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking Most Recent Version of Data if Multiple Files Are Read\n",
    "This step creates a temporary view based on the most recent version of records. By specifying key columns and ordering criteria, the notebook ensures that only the latest version of records is processed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns used for ordering if multiple files are read\n",
    "order_by_columns = [\"input_file_name DESC\", \"EventTimestamp DESC\"]\n",
    "\n",
    "# Create a temporary view with the most recent records\n",
    "dataframe.create_temp_view_with_most_recent_records(\n",
    "    spark=spark,\n",
    "    view_name=view_name,\n",
    "    key_columns=key_columns,\n",
    "    columns_of_interest=columns_of_interest,\n",
    "    order_by_columns=order_by_columns,\n",
    "    helper=helper\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write - Create Target Table If Not Exists\n",
    "This code block manages the creation of a Delta table if it doesn’t already exist. The logic handles checking table existence and executing SQL queries to create it only if needed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manage table creation if it does not exist\n",
    "table_management.manage_table_creation(\n",
    "    spark=spark,\n",
    "    destination_environment=destination_environment,\n",
    "    source_datasetidentifier=source_datasetidentifier,\n",
    "    helper=helper\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Data - Update if Exists, Insert Otherwise\n",
    "This block handles merging new data into an existing Delta table. The process identifies matching records for updates and inserts new records otherwise. Delta table versions are tracked before and after the merge to monitor changes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manage data merge\n",
    "merge_management.manage_data_merge(\n",
    "    spark=spark,\n",
    "    destination_environment=destination_environment,\n",
    "    source_datasetidentifier=source_datasetidentifier,\n",
    "    view_name=view_name,\n",
    "    key_columns=key_columns,\n",
    "    helper=helper\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return Period (from_datetime, to_datetime) Covered by Data Read\n",
    "The final step generates feedback timestamps to track the periods covered by the ingested data. This is crucial for auditing and monitoring the data pipeline’s performance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate feedback timestamps\n",
    "feedback_management.generate_feedback_timestamps(\n",
    "    spark=spark,\n",
    "    view_name=view_name,\n",
    "    feedback_column=feedback_column,\n",
    "    dbutils=dbutils,\n",
    "    helper=helper\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Completion\n",
    "The notebook exits with the final feedback output, signaling successful processing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exit the notebook with success message\n",
    "dbutils.notebook.exit(\"Notebook completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
