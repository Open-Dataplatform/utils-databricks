{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/Open-Dataplatform/utils-databricks.git@v0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from custom_utils.dp_storage.connector import mount, unmount_if_prod\n",
    "from custom_utils import adf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7522a797",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8041c2d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f0f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define source and destination configurations\n",
    "default_source_config = {\"<dataset_identifier>\": {\"type\":\"adls\", \"dataset\":\"<dataset_name>\", \"container\":\"landing\", \"account\":\"dplandingstorage\"}}\n",
    "default_destination_config = {\"<dataset_identifier>\": {\"type\":\"adls\", \"dataset\":\"<dataset_name>\", \"container\":\"uniform\", \"account\":\"dpuniformstorage\"}}\n",
    "\n",
    "# Get the configs from ADF if executed from ADF\n",
    "source_config = adf.get_source_config(dbutils, default_source_config)\n",
    "destination_config = adf.get_destination_config(dbutils, default_destination_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b29084",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_config, destination_config = mount(dbutils, source_config, destination_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4870a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get other parameters from ADF\n",
    "dbutils.widgets.removeAll()\n",
    "\n",
    "# Add or remove parameters below.\n",
    "source_folder_path = adf.get_parameter(dbutils, 'SourceFolderPath')  # Remember that it has the format \"<container>/<directory>\"\n",
    "source_filename = adf.get_parameter(dbutils, 'SourceFileName')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1422d9b",
   "metadata": {},
   "source": [
    "## Read\n",
    "Reads data from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdb501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file_path = dp_storage.reader.get_path_to_triggering_file(\n",
    "    source_folder_path,\n",
    "    source_filename,\n",
    "    config_for_triggered_dataset=source_config['TODO: dataset_identifier(not guid)']\n",
    ")\n",
    "\n",
    "df = spark.read.parquet(source_file_path)\n",
    "# Rewrite the line above, if your file is not parquet. Example for csv:\n",
    "# df = spark.read.option(\"delimiter\", \",\").csv(source_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5156013",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b159ffa9",
   "metadata": {},
   "source": [
    "## Standardize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0fa14e",
   "metadata": {},
   "source": [
    "Standardize the data here. Follow this style guide: https://github.com/palantir/pyspark-style-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e922f783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edc6b0b6",
   "metadata": {},
   "source": [
    "## Merge and upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77668ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_path = dp_storage.writer.get_destination_path(destination_config)\n",
    "database_name_databricks, table_name_databricks = dp_storage.writer.get_databricks_table_info(destination_config)\n",
    "\n",
    "# In case of full load\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .option(\"path\", destination_path) \\\n",
    "    .saveAsTable(f'{database_name_databricks}.{table_name_databricks}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4968fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always keep this at the end of your notebook\n",
    "unmount_if_prod(dbutils, source_config, destination_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
