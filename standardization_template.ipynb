{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b38ba2d2-ac55-4f6b-ba34-08a81b719653",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Data Standardization and Flattening (JSON, XML, and XLSX)\n",
    "\n",
    "This notebook is responsible for standardizing and flattening JSON, XML, and XLSX files. It reads the raw data from the landing zone, applies schemas for validation where applicable, flattens nested structures using `depth_level`, and then writes the transformed data as Delta Parquet files.\n",
    "\n",
    "## Key Features:\n",
    "- **Multi-format Support:**\n",
    "  - Reads JSON, XML, and XLSX files from the landing zone.\n",
    "  - Handles nested structures and various data types and formats.\n",
    "\n",
    "- **Schema Validation:**\n",
    "  - Applies predefined schemas for data validation:\n",
    "    - JSON: Schema files must be available at `landing/schemachecks/[datasetidentifier]/[datasetidentifier]_schema.json`.\n",
    "    - XML: Validates against XSD (XML Schema Definition) files.\n",
    "    - XLSX: Schema validation is not applicable.\n",
    "\n",
    "- **Data Flattening:**\n",
    "  - Supports flattening of nested structures in JSON and XML using `depth_level` for controlling the hierarchy level to flatten.\n",
    "  - Processes XLSX files into a structured, normalized format.\n",
    "\n",
    "- **Efficient Data Storage:**\n",
    "  - Saves the processed data as Delta Parquet files for efficient storage and querying.\n",
    "\n",
    "This notebook provides a flexible and robust framework for standardizing and preparing data for downstream analytics across multiple file formats.\n",
    "\n",
    "---\n",
    "\n",
    "## ADF Pipeline Integration\n",
    "- The notebook is designed to work seamlessly with Azure Data Factory (ADF) pipelines.\n",
    "- Parameters such as `SourceDatasetidentifier`, `SourceStorageAccount`, and other configurations are dynamically passed from the ADF pipeline at runtime.\n",
    "- External parameters passed by the pipeline automatically override the default or test configurations defined in the notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## Function Definitions\n",
    "- Most of the functions used in this notebook are part of a reusable library hosted on GitHub: [Open-Dataplatform/utils-databricks](https://github.com/Open-Dataplatform/utils-databricks).\n",
    "- Detailed descriptions of these functions, including their purpose and usage, can be found in the GitHub repository.\n",
    "- This allows you to reuse and extend the existing functionalities in your own workflows efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## Use Guide\n",
    "1. **Clone the Notebook:**\n",
    "   - Clone this notebook and use it as a template for your data processing workflows.\n",
    "\n",
    "2. **Customize the Code:**\n",
    "   - Remove unnecessary code or datasets that are not relevant to your use case.\n",
    "   - Add custom logic and transformations based on your specific requirements.\n",
    "\n",
    "3. **Test and Validate:**\n",
    "   - Use the predefined dataset configurations as an inspiration (`triton__flow_plans`, `cpx_so__nomination`, etc.) to test and validate your notebook logic.\n",
    "\n",
    "4. **Run in Production:**\n",
    "   - Integrate this notebook into an ADF pipeline and pass the required parameters dynamically for production workflows.\n",
    "\n",
    "This notebook serves as a reusable and customizable framework for handling multi-format data standardization and flattening tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab04b43b-1d9f-4b26-a5ae-260734763191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "584686ae-32de-4d1d-8c9c-7c27cf99b6a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Package Installation and Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e95cfaa7-767b-4890-a128-74d083517980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Setup: Package Installation and Management\n",
    "# ==============================================================\n",
    "\n",
    "# Purpose:\n",
    "# Manage and install essential Python packages for the Databricks project.\n",
    "# Ensures compatibility by specifying exact package versions where necessary.\n",
    "# Includes support for utilities, data processing, and XML/XLSX handling.\n",
    "\n",
    "# Step 1: Optional - Remove an existing version of the custom utility package\n",
    "# Uncomment the line below if a previous version of the utility needs to be removed.\n",
    "# %pip uninstall databricks-custom-utils -y\n",
    "\n",
    "# Step 2: Install required packages\n",
    "# The command below installs:\n",
    "# - Custom Databricks utilities (specific version from GitHub repository).\n",
    "# - Libraries for SQL parsing, Excel file handling, XML processing, and syntax highlighting.\n",
    "%pip install \\\n",
    "    git+https://github.com/Open-Dataplatform/utils-databricks.git@v1.1.3 \\\n",
    "    sqlparse \\\n",
    "    openpyxl \\\n",
    "    lxml \\\n",
    "    xmlschema \\\n",
    "    pygments\n",
    "\n",
    "\"\"\"\n",
    "Package Details:\n",
    "- `utils-databricks`: Custom utilities for extended functionality in Databricks.\n",
    "- `sqlparse`: SQL query parsing and formatting library.\n",
    "- `openpyxl`: Library for handling Excel (XLSX) files.\n",
    "- `lxml`: Robust library for processing XML and HTML files.\n",
    "- `xmlschema`: Tools for XML schema validation and conversion.\n",
    "- `pygments`: Syntax highlighting for code snippets in logs or reports.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44a24c24-46c3-4030-808e-1efc2e2a5e8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Initialization and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dba6327-e984-4f99-a3f3-bb3ff22af9ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Initialize Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f10b900-b696-4643-bbd4-a732eb58e80d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Initialize Logger\n",
    "# ==============================================================\n",
    "\n",
    "# Purpose:\n",
    "# Set up a custom logger for detailed logging and debugging throughout the notebook.\n",
    "# The logger offers advanced features, including:\n",
    "# - Debug-level logging for in-depth insights during execution.\n",
    "# - Block-style logging for structured, readable logs.\n",
    "# - Syntax highlighting for SQL queries and Python code in logs.\n",
    "\n",
    "# Step 1: Import the Logger class from the custom utilities package\n",
    "from custom_utils.logging.logger import Logger\n",
    "\n",
    "# Step 2: Initialize the Logger instance\n",
    "# - `debug=True` enables detailed logs, useful for troubleshooting and analysis.\n",
    "logger = Logger(debug=True)\n",
    "\n",
    "# Log the initialization success\n",
    "logger.log_message(\"Logger initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4ad8321-493b-4a38-8d1c-0b6d5d171a4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Initializes widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4f4ad50-4dc3-437e-bc6a-5d44fce4fb1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Widget Initialization and Test Configuration\n",
    "# ==============================================================\n",
    "\n",
    "# Purpose:\n",
    "# Set up widgets for testing the \"xxx\" dataset using XML files.\n",
    "# The widgets enable dynamic configuration of critical parameters, including file type,\n",
    "# storage accounts, containers, dataset identifiers, and XML-specific options.\n",
    "\n",
    "# Step 1: Initialize File Type dropdown with supported options\n",
    "dbutils.widgets.text(\"FileType\", \"json\", \"File Type\")\n",
    "\n",
    "# Step 2: Define Source and Destination Storage Accounts\n",
    "dbutils.widgets.text(\"SourceStorageAccount\", \"xxx\", \"Source Storage Account\")\n",
    "dbutils.widgets.text(\"DestinationStorageAccount\", \"xxx\", \"Destination Storage Account\")\n",
    "\n",
    "# Step 3: Configure Source Container and Dataset Identifier\n",
    "dbutils.widgets.text(\"SourceContainer\", \"xxx\", \"Source Container\")\n",
    "dbutils.widgets.text(\"SourceDatasetidentifier\", \"xxx\", \"Source Datasetidentifier\")\n",
    "\n",
    "# Step 4: Specify Source File Name and Key Columns\n",
    "dbutils.widgets.text(\"SourceFileName\", \"xxx*\", \"Source File Name\")\n",
    "dbutils.widgets.text(\"KeyColumns\", \"xxx, yyy\", \"Key Columns\")\n",
    "\n",
    "# Step 5: Set Additional Parameters for Feedback Column, Depth Level, Schema Folder, and XML Root Name\n",
    "dbutils.widgets.text(\"FeedbackColumn\", \"xxx\", \"Feedback Column\")\n",
    "dbutils.widgets.text(\"DepthLevel\", \"x\", \"Depth Level\")\n",
    "dbutils.widgets.text(\"SchemaFolderName\", \"xxx\", \"Schema Folder Name\")\n",
    "\n",
    "# Log the initialization success\n",
    "logger.log_message(\"Widget initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20549033-495b-4d3f-8d4b-e1a23960770b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Initialize notebook and retrieve parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80758c56-f950-42e6-8ce1-4e3fc852c55d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Initialize Notebook and Retrieve Parameters\n",
    "# ==============================================================\n",
    "\n",
    "# Purpose:\n",
    "# Set up the notebook by initializing its configuration and retrieving essential parameters.\n",
    "# This ensures centralized management of settings and enables efficient debugging\n",
    "# through a consistent configuration framework.\n",
    "\n",
    "# Step 1: Import the Config class from the custom utilities package\n",
    "from custom_utils.config.config import Config\n",
    "\n",
    "# Step 2: Initialize the Config object\n",
    "# - Pass `dbutils` for accessing Databricks workspace resources.\n",
    "# - Set `debug=False` to disable verbose debug logs for cleaner execution.\n",
    "config = Config.initialize(dbutils=dbutils, debug=False)\n",
    "\n",
    "# Step 3: Unpack configuration parameters\n",
    "# - Extracts configuration values into the notebook's global scope.\n",
    "# - This simplifies access to parameters by making them available as standard variables.\n",
    "config.unpack(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89a66346-a20e-422c-a79e-bd4f39a66035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Verify paths and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aaa71031-35b8-4fd1-906b-5216a6ec3175",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Verify Paths and Files\n",
    "# ==============================================================\n",
    "\n",
    "# Purpose:\n",
    "# Validate the required paths and files to ensure all necessary resources \n",
    "# are available for processing. This pre-check prevents runtime errors \n",
    "# by identifying and addressing issues early in the notebook execution.\n",
    "\n",
    "# Step 1: Import the Validator class from the custom utilities package\n",
    "from custom_utils.validation.validation import Validator\n",
    "\n",
    "# Step 2: Initialize the Validator\n",
    "# - Pass `config` to access path and file parameters from the configuration.\n",
    "# - Set `debug=False` for standard validation logging without verbose output.\n",
    "validator = Validator(config=config, debug=False)\n",
    "\n",
    "# Step 3: Unpack validation parameters\n",
    "# - Extracts validation-related parameters into the notebook's global scope.\n",
    "validator.unpack(globals())\n",
    "\n",
    "# Step 4: Perform validation and check for an exit flag\n",
    "# - If critical validation fails, the notebook execution is terminated.\n",
    "validator.check_and_exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a185b08-95bf-44ee-a2e8-b1c965425725",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exit the Notebook if Validation Fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1be00119-df2f-4b9b-9575-e338a92e8261",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Exit the Notebook if Validation Fails\n",
    "# ==============================================================\n",
    "\n",
    "# Purpose:\n",
    "# Stop notebook execution gracefully if critical validation checks fail.\n",
    "# If validation passes, continue processing with a confirmation message.\n",
    "\n",
    "# Step 1: Check for an exit condition flagged by the Validator\n",
    "if Validator.exit_notebook:\n",
    "    # Step 2: Log the exit message using the logger\n",
    "    # - Provides context on why the notebook execution is being terminated.\n",
    "    logger.log_error(Validator.exit_notebook_message)\n",
    "    \n",
    "    # Step 3: Exit the notebook with a descriptive message\n",
    "    # - Uses Databricks utilities to terminate execution cleanly.\n",
    "    dbutils.notebook.exit(f\"Notebook exited: {Validator.exit_notebook_message}\")\n",
    "else:\n",
    "    # Step 4: Log a success message if validation passed\n",
    "    # - Confirms the notebook will continue execution.\n",
    "    logger.log_message(\"Validation passed. The notebook is proceeding without exiting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9230c35-0c30-43dc-87dd-647eb75ba4dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Processing Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8639e580-aa54-4fc1-b352-25bea590d603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Flattening and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "822cab05-0613-4d35-94a5-3d8686db5adb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Processing Workflow - Flattening and Processing\n",
    "# ==============================================================\n",
    "\n",
    "# Purpose:\n",
    "# This workflow simplifies hierarchical data by flattening it, renaming columns, \n",
    "# casting data types, and applying dataset-specific transformations.\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, expr, unix_timestamp, from_unixtime, to_utc_timestamp\n",
    "from custom_utils.transformations.dataframe import DataFrameTransformer\n",
    "\n",
    "# Initialize the DataFrameTransformer\n",
    "transformer = DataFrameTransformer(config=config, debug=False)\n",
    "\n",
    "try:\n",
    "    # Step 1: Process and flatten the data\n",
    "    df_initial, df_flattened = transformer.process_and_flatten_data(depth_level=depth_level)\n",
    "\n",
    "    # Step 3: Display the flattened DataFrame\n",
    "    logger.log_block(\"Displaying the flattened DataFrame.\")\n",
    "    display(df_flattened)\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle errors gracefully\n",
    "    logger.log_error(f\"Error during processing: {str(e)}\")\n",
    "    dbutils.notebook.exit(f\"Processing failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1df0ac85-008d-4c22-83d5-b219bb6709aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Quality check "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1153c031-f268-4c07-8c0f-91707e842934",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Perform Quality Check and Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a3d2553-190f-49ff-8141-93774b4af9d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Quality Check - Perform Quality Check and Remove Duplicates\n",
    "# ==============================================================\n",
    "\n",
    "# Purpose:\n",
    "# This section performs data quality checks to ensure:\n",
    "# - The integrity, accuracy, and consistency of the processed data.\n",
    "# - Duplicate records are identified and optionally removed.\n",
    "# - Additional quality checks (e.g., null value checks, value range checks) are executed.\n",
    "\n",
    "from custom_utils.quality.quality import DataQualityManager\n",
    "\n",
    "# Step 1: Initialize the DataQualityManager\n",
    "# - This class manages all quality check operations and logs relevant information.\n",
    "quality_manager = DataQualityManager(logger=logger, debug=True)\n",
    "\n",
    "# Step 2: Log available quality checks\n",
    "# - Provides an overview of checks supported by the quality manager for user reference.\n",
    "quality_manager.describe_available_checks()\n",
    "\n",
    "# Step 3: Execute data quality checks on the flattened DataFrame\n",
    "try:\n",
    "    # Perform quality checks with the following configurations:\n",
    "    cleaned_data_view = quality_manager.perform_data_quality_checks(\n",
    "        spark=spark,  # Required: Spark session.\n",
    "        df=df_flattened,  # Required: DataFrame to perform quality checks on.\n",
    "        \n",
    "        # Key columns for partitioning and duplicate checking.\n",
    "        # - Required parameter to identify unique records in the dataset.\n",
    "        key_columns=key_columns,\n",
    "        \n",
    "        # Optional: Columns for ordering within partitions (e.g., to select the latest record).\n",
    "        # - Defaults to `key_columns` if not provided.\n",
    "        order_by=\"input_file_name\",\n",
    "        \n",
    "        # Optional: Column to use for duplicate removal ordering.\n",
    "        # - If not provided, falls back to `key_columns`.\n",
    "        feedback_column=\"input_file_name\",\n",
    "        \n",
    "        # Optional: Column for referential integrity check against a reference DataFrame.\n",
    "        # - Ensures that foreign key relationships are maintained.\n",
    "        join_column=key_columns,\n",
    "        \n",
    "        # Optional: Exclude specified columns from the final DataFrame.\n",
    "        # - For example, `input_file_name` is excluded to avoid irrelevant data in output.\n",
    "        columns_to_exclude=[\"input_file_name\"],\n",
    "        \n",
    "        # Optional: Specify whether to use Python or SQL syntax for quality checks.\n",
    "        # - Default is SQL-based for optimized performance.\n",
    "        use_python=False\n",
    "    )\n",
    "\n",
    "    # Description of Arguments:\n",
    "    # - `spark`: Spark session used for distributed processing (required).\n",
    "    # - `df`: The DataFrame on which quality checks are performed (required).\n",
    "    # - `key_columns`: Columns used for identifying unique records (required).\n",
    "    # - `order_by`: Columns for ordering within partitions (optional; defaults to `key_columns`).\n",
    "    # - `feedback_column`: Column used for ordering duplicates (optional; falls back to `key_columns`).\n",
    "    # - `join_column`: Column for referential integrity validation (optional).\n",
    "    # - `columns_to_exclude`: List of columns to exclude from the final DataFrame (optional).\n",
    "    # - `use_python`: Boolean flag to select Python-based or SQL-based operations (optional).\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle any errors during the quality check process\n",
    "    logger.log_error(f\"Error during quality check: {str(e)}\")\n",
    "    raise RuntimeError(f\"Quality check failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47917611-ad2c-47de-990a-d37364e7bd9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Unified Data Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4822f770-be3c-4b54-b369-2541e8ae64bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Table Creation and Data Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9532625b-f890-42c1-b9e8-f6d76242bf23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Unified Data Management: Table Creation and Data Merging\n",
    "# ==============================================================\n",
    "\n",
    "# Purpose:\n",
    "# This section handles the creation of destination tables and merges\n",
    "# processed data into the respective storage location. It ensures:\n",
    "# - Data is written to a unified storage with consistent formatting.\n",
    "# - Merging supports updates, inserts, and deletions seamlessly.\n",
    "# - Storage operations are managed efficiently with robust logging.\n",
    "\n",
    "from custom_utils.catalog.catalog_utils import DataStorageManager\n",
    "\n",
    "# Step 1: Initialize the DataStorageManager\n",
    "# - Manages operations related to data storage and merging.\n",
    "# - Includes detailed logging and debugging capabilities.\n",
    "storage_manager = DataStorageManager(logger=logger, debug=True)\n",
    "\n",
    "# Step 2: Perform the data storage operation\n",
    "try:\n",
    "    # Manage data operation with the following configurations (these parameters are defined in the configuration and passed as parameters to the function, and can be overridden if necessary):\n",
    "    # The used variables (cleaned_data_view, key_columns, etc.) are created and assigned by call to config.unpack(globals()) above, under section \"Initialize Notebook and Retrieve Parameters\"\n",
    "    storage_manager.manage_data_operation(\n",
    "        spark=spark,  # Required: Spark session for executing SQL or DataFrame operations.\n",
    "        dbutils=dbutils,  # Required: Databricks utilities for interacting with storage.\n",
    "\n",
    "        # Name of the cleaned data view containing the processed DataFrame.\n",
    "        # - Required parameter that holds the cleaned and transformed data.\n",
    "        cleaned_data_view=cleaned_data_view,\n",
    "\n",
    "        # Key columns used for matching records during the merge operation.\n",
    "        # - Required parameter to ensure data consistency during updates.\n",
    "        key_columns=key_columns,\n",
    "\n",
    "        # Destination folder path for storing data as Delta files.\n",
    "        # - Optional: If not provided, a default path defined in the configuration is used.\n",
    "        destination_folder_path=destination_data_folder_path,\n",
    "\n",
    "        # Target database or environment for storing the data.\n",
    "        # - Optional: Overrides the default destination environment if provided.\n",
    "        destination_environment=destination_environment,\n",
    "\n",
    "        # Target table name or identifier for the source dataset.\n",
    "        # - Optional: Allows dynamic specification of the target table for merging.\n",
    "        source_datasetidentifier=source_datasetidentifier,\n",
    "\n",
    "        # Boolean flag to select SQL-based (default) or Python DataFrame-based operations.\n",
    "        # - Optional: Default is `False` to prioritize SQL for performance.\n",
    "        use_python=False\n",
    "    )\n",
    "\n",
    "    # Description of Arguments:\n",
    "    # - `spark`: Active Spark session (required).\n",
    "    # - `dbutils`: Databricks utilities object for workspace interaction (required).\n",
    "    # - `cleaned_data_view`: Name of the view containing cleaned data (required).\n",
    "    # - `key_columns`: Columns used for identifying unique records during merge (required).\n",
    "    # - `destination_folder_path`: Override for the destination folder (optional).\n",
    "    # - `destination_environment`: Override for the target database/environment (optional).\n",
    "    # - `source_datasetidentifier`: Override for the source dataset/table identifier (optional).\n",
    "    # - `use_python`: Boolean flag for using Python or SQL operations (optional).\n",
    "\n",
    "    # Log success\n",
    "    logger.log_message(\"Data successfully written and merged into the destination table.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Step 3: Handle errors during the data storage process\n",
    "    # - Logs the error details and raises a RuntimeError to terminate execution.\n",
    "    logger.log_error(f\"Error during data storage operation: {str(e)}\")\n",
    "    raise RuntimeError(f\"Data storage operation failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0df06a97-1bc0-482c-843c-149155def8a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Finishing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6f9eaf4-87e1-4016-a43b-19284126fcc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Return period (from_datetime, to_datetime) covered by data read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7b590b0-5501-487a-b037-06ccaf16d5ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Finishing - Return Period Covered by Data Read\n",
    "# ==============================================================\n",
    "\n",
    "# Purpose:\n",
    "# This section generates the feedback timestamps, providing the time \n",
    "# period covered by the processed and stored data. It calculates \n",
    "# `from_datetime` and `to_datetime` based on the data in the cleaned \n",
    "# data view.\n",
    "\n",
    "# Step 1: Generate feedback timestamps\n",
    "try:\n",
    "    # Call the `generate_feedback_timestamps` function with the following:\n",
    "    # - The active Spark session to access the view.\n",
    "    # - The name of the cleaned data view containing processed data.\n",
    "    # - The `feedback_column` for timestamp calculations (optional).\n",
    "    # - The `key_columns` for grouping (optional).\n",
    "    notebook_output = storage_manager.generate_feedback_timestamps(\n",
    "        spark=spark,  # Active Spark session\n",
    "        view_name=cleaned_data_view,  # The view containing cleaned and processed data\n",
    "        feedback_column=feedback_column,  # Column used for identifying feedback periods\n",
    "        key_columns=key_columns  # Key columns for grouping and extracting timestamp bounds\n",
    "    )\n",
    "\n",
    "    # Log the successful generation of feedback timestamps\n",
    "    logger.log_message(\"Feedback timestamps successfully generated.\", level=\"info\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle errors during feedback timestamp generation\n",
    "    logger.log_error(f\"Error generating feedback timestamps: {str(e)}\")\n",
    "    raise RuntimeError(f\"Failed to generate feedback timestamps: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91635a76-227b-436e-8d70-febf660305d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exit the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5b4800f-c541-4c25-ab49-32ea7814dfa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Exit the Notebook\n",
    "# ==============================================================\n",
    "\n",
    "# Purpose:\n",
    "# Finalize the notebook execution by exiting and returning the output.\n",
    "# The output provides a summary of the period covered by the processed data,\n",
    "# ensuring a clear handoff to any downstream workflows.\n",
    "\n",
    "# Step 1: Exit the notebook with the generated output\n",
    "try:\n",
    "    # Use dbutils to exit the notebook gracefully\n",
    "    # - The `notebook_output` contains feedback timestamps or relevant results.\n",
    "    dbutils.notebook.exit(notebook_output)\n",
    "\n",
    "    # Log the successful exit for tracking and debugging purposes\n",
    "    logger.log_message(f\"Notebook exited successfully with output: {notebook_output}\", level=\"info\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle errors during the exit process\n",
    "    # - Logs the error and raises a RuntimeError to signal failure.\n",
    "    logger.log_error(f\"Error during notebook exit: {str(e)}\")\n",
    "    raise RuntimeError(f\"Failed to exit the notebook: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "s-data_group__entityname",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
