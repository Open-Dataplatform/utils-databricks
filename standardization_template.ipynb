{
    "metadata": {
        "language_info": {
            "name": "python"
        },
        "application/vnd.databricks.v1+notebook": {
            "computePreferences": null,
            "dashboards": [],
            "environmentMetadata": null,
            "language": "python",
            "notebookMetadata": {
                "pythonIndentUnit": 4
            },
            "notebookName": "s-data_group__entityname",
            "widgets": {}
        },
        "extensions": {
            "azuredatastudio": {
                "version": 1,
                "views": []
            }
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "# Data Standardization and Flattening (JSON, XML, and XLSX)\n",
                "This notebook is responsible for standardizing and flattening JSON, XML, and XLSX files. It reads the raw data from the landing zone, applies schemas for validation where applicable, flattens nested structures using `depth_level`, and then writes the transformed data as Delta Parquet files.\n",
                "\n",
                "## Key Features:\n",
                "- **Multi-format Support:**\n",
                "  - Reads JSON, XML, and XLSX files from the landing zone.\n",
                "  - Handles nested structures and various data types in all formats.\n",
                "\n",
                "- **Schema Validation:**\n",
                "  - Applies predefined schemas for data validation:\n",
                "    - JSON: Schema files must be available at `landing/schemachecks/[datasetidentifier]/[datasetidentifier]_schema.json`.\n",
                "    - XML: Validates against XSD (XML Schema Definition) files.\n",
                "    - XLSX: Schema validation is not applicable.\n",
                "\n",
                "- **Data Flattening:**\n",
                "  - Supports flattening of nested structures in JSON and XML using `depth_level` for controlling the hierarchy level to flatten.\n",
                "  - Processes XLSX files into a structured, normalized format.\n",
                "\n",
                "- **Efficient Data Storage:**\n",
                "  - Saves the processed data as Delta Parquet files for efficient storage and querying.\n",
                "\n",
                "This notebook provides a flexible and robust framework for standardizing and preparing data for downstream analytics across multiple file formats."
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "936f07be-db8a-4ab9-8a49-b5d16806a4f1",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "f9583efb-3a7d-4218-8be9-7437434c4bb2"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Widget Initialization and Test Configuration"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "88d0d9a0-3f4f-492a-ad35-4942ac2c6ecb",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "fc01956d-6705-41f3-b6fe-2cf85be27271"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Clear all existing widgets"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "2912f1cb-1bc5-4be2-9082-53d461915ced",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "e7d301aa-40a5-401a-8d09-fb98e7f7b4bb"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Widget Initialization and Test Configuration: Clear All Widgets\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# Ensure a clean slate for widget initialization by removing all existing widgets.\n",
                "# This step prevents duplication or conflicts with previously defined widgets.\n",
                "\n",
                "# Step 1: Remove all existing widgets\n",
                "try:\n",
                "    dbutils.widgets.removeAll()\n",
                "    logger.log_message(\"All existing widgets removed successfully.\")\n",
                "except Exception as e:\n",
                "    logger.log_error(f\"Error during widget cleanup: {str(e)}\")\n",
                "    raise RuntimeError(f\"Failed to clear widgets: {str(e)}\")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "collapsed": true,
                    "inputWidgets": {},
                    "nuid": "b8232fa6-bb6d-4e6a-9e62-8456f5895ebe",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "44b7d700-2e60-4e8c-a8dc-9cada8a89c21"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Triton Flow Plans (JSON Example)"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "13a8aae5-95af-4190-836e-bc504066f3be",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "7c78ea42-1a2e-49c4-a53c-b381a3ae3470"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Widget Initialization and Test Configuration: Triton Flow Plans (JSON Example)\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# Set up widgets for testing the \"triton__flow_plans\" dataset using JSON files.\n",
                "# The widgets enable dynamic configuration of key parameters such as file type,\n",
                "# storage accounts, containers, dataset identifiers, and processing options.\n",
                "\n",
                "# Step 1: Clear all existing widgets to ensure no duplication or conflict\n",
                "try:\n",
                "    dbutils.widgets.removeAll()\n",
                "    logger.log_message(\"All existing widgets cleared successfully.\")\n",
                "except Exception as e:\n",
                "    logger.log_error(f\"Error clearing widgets: {str(e)}\")\n",
                "    raise RuntimeError(f\"Failed to initialize widgets: {str(e)}\")\n",
                "\n",
                "# Step 2: Initialize File Type dropdown with supported options\n",
                "dbutils.widgets.dropdown(\"FileType\", \"json\", [\"json\", \"xlsx\", \"xml\"], \"File Type\")\n",
                "\n",
                "# Step 3: Define Source and Destination Storage Accounts\n",
                "dbutils.widgets.text(\"SourceStorageAccount\", \"dplandingstoragetest\", \"Source Storage Account\")\n",
                "dbutils.widgets.text(\"DestinationStorageAccount\", \"dpuniformstoragetest\", \"Destination Storage Account\")\n",
                "\n",
                "# Step 4: Configure Source Container and Dataset Identifier\n",
                "dbutils.widgets.text(\"SourceContainer\", \"landing\", \"Source Container\")\n",
                "dbutils.widgets.text(\"SourceDatasetidentifier\", \"triton__flow_plans\", \"Source Datasetidentifier\")\n",
                "\n",
                "# Step 5: Specify Source File Name and Key Columns\n",
                "dbutils.widgets.text(\"SourceFileName\", \"triton__flow_plans-202408*\", \"Source File Name\")\n",
                "dbutils.widgets.text(\"KeyColumns\", \"Guid\", \"Key Columns\")\n",
                "\n",
                "# Step 6: Set Feedback Column, Flattening Depth Level, and Schema Folder Name\n",
                "dbutils.widgets.text(\"FeedbackColumn\", \"EventTimestamp\", \"Feedback Column\")\n",
                "dbutils.widgets.text(\"DepthLevel\", \"1\", \"Depth Level\")\n",
                "dbutils.widgets.text(\"SchemaFolderName\", \"schemachecks\", \"Schema Folder Name\")\n",
                "\n",
                "# Log a summary of widget initialization\n",
                "logger.log_block(\"Widget Initialization Summary\", [\n",
                "    \"FileType: json (default)\",\n",
                "    \"SourceStorageAccount: dplandingstoragetest\",\n",
                "    \"DestinationStorageAccount: dpuniformstoragetest\",\n",
                "    \"SourceContainer: landing\",\n",
                "    \"SourceDatasetidentifier: triton__flow_plans\",\n",
                "    \"SourceFileName: triton__flow_plans-202408*\",\n",
                "    \"KeyColumns: Guid\",\n",
                "    \"FeedbackColumn: EventTimestamp\",\n",
                "    \"DepthLevel: 1\",\n",
                "    \"SchemaFolderName: schemachecks\"\n",
                "])"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "collapsed": true,
                    "inputWidgets": {},
                    "nuid": "27401e6c-61da-440c-bb9e-b70388333f75",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "71c88796-b0e3-4491-8713-a9fa5c267dc4"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### CPX SO Nomination (JSON Example)"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "4e990307-b98a-424f-8d3e-2ee98e03f29d",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "72638a52-c3f2-4f31-a6f6-6d0c4deeb2db"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Widget Initialization and Test Configuration: CPX SO Nomination (JSON Example)\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# Set up widgets for testing the \"cpx_so__nomination\" dataset using JSON files.\n",
                "# The widgets enable dynamic configuration of critical parameters such as file type,\n",
                "# storage accounts, containers, dataset identifiers, and processing options.\n",
                "\n",
                "# Step 1: Clear all existing widgets to ensure no duplication or conflict\n",
                "try:\n",
                "    dbutils.widgets.removeAll()\n",
                "    logger.log_message(\"All existing widgets cleared successfully.\")\n",
                "except Exception as e:\n",
                "    logger.log_error(f\"Error clearing widgets: {str(e)}\")\n",
                "    raise RuntimeError(f\"Failed to initialize widgets: {str(e)}\")\n",
                "\n",
                "# Step 2: Initialize File Type dropdown with supported options\n",
                "dbutils.widgets.dropdown(\"FileType\", \"json\", [\"json\", \"xlsx\", \"xml\"], \"File Type\")\n",
                "\n",
                "# Step 3: Define Source and Destination Storage Accounts\n",
                "dbutils.widgets.text(\"SourceStorageAccount\", \"dplandingstoragetest\", \"Source Storage Account\")\n",
                "dbutils.widgets.text(\"DestinationStorageAccount\", \"dpuniformstoragetest\", \"Destination Storage Account\")\n",
                "\n",
                "# Step 4: Configure Source Container and Dataset Identifier\n",
                "dbutils.widgets.text(\"SourceContainer\", \"landing\", \"Source Container\")\n",
                "dbutils.widgets.text(\"SourceDatasetidentifier\", \"cpx_so__nomination\", \"Source Datasetidentifier\")\n",
                "\n",
                "# Step 5: Specify Source File Name and Key Columns\n",
                "dbutils.widgets.text(\"SourceFileName\", \"cpx_so__nomination-20241127T21*\", \"Source File Name\")\n",
                "dbutils.widgets.text(\n",
                "    \"KeyColumns\", \n",
                "    \"flows_accountInternal_code, flows_accountExternal_code, flows_location_code, flows_direction, \"\n",
                "    \"flows_periods_validityPeriod_begin, flows_periods_validityPeriod_end\", \n",
                "    \"Key Columns\"\n",
                ")\n",
                "\n",
                "# Step 6: Set Feedback Column, Flattening Depth Level, and Schema Folder Name\n",
                "dbutils.widgets.text(\"FeedbackColumn\", \"dateCreated\", \"Feedback Column\")\n",
                "dbutils.widgets.text(\"DepthLevel\", \"\", \"Depth Level\")\n",
                "dbutils.widgets.text(\"SchemaFolderName\", \"schemachecks\", \"Schema Folder Name\")\n",
                "\n",
                "# Log a summary of widget initialization\n",
                "logger.log_block(\"Widget Initialization Summary\", [\n",
                "    \"FileType: json (default)\",\n",
                "    \"SourceStorageAccount: dplandingstoragetest\",\n",
                "    \"DestinationStorageAccount: dpuniformstoragetest\",\n",
                "    \"SourceContainer: landing\",\n",
                "    \"SourceDatasetidentifier: cpx_so__nomination\",\n",
                "    \"SourceFileName: cpx_so__nomination-20241127T21*\",\n",
                "    \"KeyColumns: flows_accountInternal_code, flows_accountExternal_code, flows_location_code, flows_direction, \"\n",
                "    \"flows_periods_validityPeriod_begin, flows_periods_validityPeriod_end\",\n",
                "    \"FeedbackColumn: dateCreated\",\n",
                "    \"DepthLevel: Not Specified\",\n",
                "    \"SchemaFolderName: schemachecks\"\n",
                "])"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "collapsed": true,
                    "inputWidgets": {},
                    "nuid": "d8892c28-8fe3-4923-a616-b3129850affa",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "520435b5-ddfa-4bab-818b-c055f838ad70"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### DDP EM Day-Ahead Flows NEMO (XML Example)"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "e1688ad6-c1ca-47b6-aca1-0e43d815c75c",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "3feb637b-ed36-418e-9a7d-9a98cd31a755"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Widget Initialization and Test Configuration: DDP EM Day-Ahead Flows NEMO (XML Example)\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# Set up widgets for testing the \"ddp_em__dayahead_flows_nemo\" dataset using XML files.\n",
                "# The widgets enable dynamic configuration of critical parameters, including file type,\n",
                "# storage accounts, containers, dataset identifiers, and XML-specific options.\n",
                "\n",
                "# Step 1: Clear all existing widgets to ensure no duplication or conflicts\n",
                "try:\n",
                "    dbutils.widgets.removeAll()\n",
                "    logger.log_message(\"All existing widgets cleared successfully.\")\n",
                "except Exception as e:\n",
                "    logger.log_error(f\"Error clearing widgets: {str(e)}\")\n",
                "    raise RuntimeError(f\"Failed to initialize widgets: {str(e)}\")\n",
                "\n",
                "# Step 2: Initialize File Type dropdown with supported options\n",
                "dbutils.widgets.dropdown(\"FileType\", \"xml\", [\"json\", \"xlsx\", \"xml\"], \"File Type\")\n",
                "\n",
                "# Step 3: Define Source and Destination Storage Accounts\n",
                "dbutils.widgets.text(\"SourceStorageAccount\", \"dplandingstoragetest\", \"Source Storage Account\")\n",
                "dbutils.widgets.text(\"DestinationStorageAccount\", \"dpuniformstoragetest\", \"Destination Storage Account\")\n",
                "\n",
                "# Step 4: Configure Source Container and Dataset Identifier\n",
                "dbutils.widgets.text(\"SourceContainer\", \"landing\", \"Source Container\")\n",
                "dbutils.widgets.text(\"SourceDatasetidentifier\", \"ddp_em__dayahead_flows_nemo\", \"Source Datasetidentifier\")\n",
                "\n",
                "# Step 5: Specify Source File Name and Key Columns\n",
                "dbutils.widgets.text(\"SourceFileName\", \"ddp_em__dayahead_flows_nemo-202405*\", \"Source File Name\")\n",
                "dbutils.widgets.text(\n",
                "    \"KeyColumns\", \n",
                "    \"TimeSeries_mRID, TimeSeries_Period_timeInterval_start, TimeSeries_Period_Point_position\", \n",
                "    \"Key Columns\"\n",
                ")\n",
                "\n",
                "# Step 6: Set Additional Parameters for Feedback Column, Depth Level, Schema Folder, and XML Root Name\n",
                "dbutils.widgets.text(\"FeedbackColumn\", \"timeseries_timestamp\", \"Feedback Column\")\n",
                "dbutils.widgets.text(\"DepthLevel\", \"1\", \"Depth Level\")\n",
                "dbutils.widgets.text(\"SchemaFolderName\", \"schemachecks\", \"Schema Folder Name\")\n",
                "dbutils.widgets.text(\"XmlRootName\", \"Schedule_MarketDocument\", \"XML Root Name\")\n",
                "\n",
                "# Log a summary of widget initialization\n",
                "logger.log_block(\"Widget Initialization Summary\", [\n",
                "    \"FileType: xml (default)\",\n",
                "    \"SourceStorageAccount: dplandingstoragetest\",\n",
                "    \"DestinationStorageAccount: dpuniformstoragetest\",\n",
                "    \"SourceContainer: landing\",\n",
                "    \"SourceDatasetidentifier: ddp_em__dayahead_flows_nemo\",\n",
                "    \"SourceFileName: ddp_em__dayahead_flows_nemo-202405*\",\n",
                "    \"KeyColumns: TimeSeries_mRID, TimeSeries_Period_timeInterval_start, TimeSeries_Period_Point_position\",\n",
                "    \"FeedbackColumn: timeseries_timestamp\",\n",
                "    \"DepthLevel: 1\",\n",
                "    \"SchemaFolderName: schemachecks\",\n",
                "    \"XML Root Name: Schedule_MarketDocument\"\n",
                "])"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "collapsed": true,
                    "inputWidgets": {},
                    "nuid": "494b3d88-ce65-45be-874c-2db5c0178bd4",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "010ddfd0-745e-423b-8e1d-31fba6955a2c"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### DDP CM mFRR Settlement (XML Example)"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "a33348bb-29ba-48fe-ad12-f60a07e3d116",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "259fb02e-3e71-4dac-bb63-0968f74a588b"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Widget Initialization and Test Configuration: DDP CM mFRR Settlement (XML Example)\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# Set up widgets for testing the \"ddp_cm__mfrr_settlement\" dataset using XML files.\n",
                "# The widgets enable dynamic configuration of critical parameters, including file type,\n",
                "# storage accounts, containers, dataset identifiers, and XML-specific options.\n",
                "\n",
                "# Step 1: Clear all existing widgets to ensure no duplication or conflicts\n",
                "try:\n",
                "    dbutils.widgets.removeAll()\n",
                "    logger.log_message(\"All existing widgets cleared successfully.\")\n",
                "except Exception as e:\n",
                "    logger.log_error(f\"Error clearing widgets: {str(e)}\")\n",
                "    raise RuntimeError(f\"Failed to initialize widgets: {str(e)}\")\n",
                "\n",
                "# Step 2: Initialize File Type dropdown with supported options\n",
                "dbutils.widgets.dropdown(\"FileType\", \"xml\", [\"json\", \"xlsx\", \"xml\"], \"File Type\")\n",
                "\n",
                "# Step 3: Define Source and Destination Storage Accounts\n",
                "dbutils.widgets.text(\"SourceStorageAccount\", \"dplandingstoragetest\", \"Source Storage Account\")\n",
                "dbutils.widgets.text(\"DestinationStorageAccount\", \"dpuniformstoragetest\", \"Destination Storage Account\")\n",
                "\n",
                "# Step 4: Configure Source Container and Dataset Identifier\n",
                "dbutils.widgets.text(\"SourceContainer\", \"landing\", \"Source Container\")\n",
                "dbutils.widgets.text(\"SourceDatasetidentifier\", \"ddp_cm__mfrr_settlement\", \"Source Datasetidentifier\")\n",
                "\n",
                "# Step 5: Specify Source File Name and Key Columns\n",
                "dbutils.widgets.text(\"SourceFileName\", \"*\", \"Source File Name\")\n",
                "dbutils.widgets.text(\n",
                "    \"KeyColumns\", \n",
                "    \"mRID, TimeSeries_mRID, TimeSeries_Period_timeInterval_start, TimeSeries_Period_Point_position, TimeSeries_Period_resolution\", \n",
                "    \"Key Columns\"\n",
                ")\n",
                "\n",
                "# Step 6: Set Additional Parameters for Feedback Column, Depth Level, Schema Folder, and XML Root Name\n",
                "dbutils.widgets.text(\"FeedbackColumn\", \"input_file_name\", \"Feedback Column\")\n",
                "dbutils.widgets.text(\"DepthLevel\", \"\", \"Depth Level\")\n",
                "dbutils.widgets.text(\"SchemaFolderName\", \"schemachecks\", \"Schema Folder Name\")\n",
                "dbutils.widgets.text(\"XmlRootName\", \"ReserveAllocationResult_MarketDocument\", \"XML Root Name\")\n",
                "\n",
                "# Log a summary of widget initialization\n",
                "logger.log_block(\"Widget Initialization Summary\", [\n",
                "    \"FileType: xml (default)\",\n",
                "    \"SourceStorageAccount: dplandingstoragetest\",\n",
                "    \"DestinationStorageAccount: dpuniformstoragetest\",\n",
                "    \"SourceContainer: landing\",\n",
                "    \"SourceDatasetidentifier: ddp_cm__mfrr_settlement\",\n",
                "    \"SourceFileName: *\",\n",
                "    \"KeyColumns: mRID, TimeSeries_mRID, TimeSeries_Period_timeInterval_start, TimeSeries_Period_Point_position, TimeSeries_Period_resolution\",\n",
                "    \"FeedbackColumn: input_file_name\",\n",
                "    \"DepthLevel: (none specified)\",\n",
                "    \"SchemaFolderName: schemachecks\",\n",
                "    \"XML Root Name: ReserveAllocationResult_MarketDocument\"\n",
                "])"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "collapsed": true,
                    "inputWidgets": {},
                    "nuid": "c638699e-f310-42c7-a526-bb339ec6f1da",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "ebf725bc-1f65-445c-8d7d-a529ac52e986"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### PLUTO PC Units SCADA MW (XLSX Example)"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "43342a40-e5c9-4225-8558-08e6d252bdc3",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "77e43fd0-e982-44f5-8602-84473e7ffeb8"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Widget Initialization and Test Configuration: PLUTO PC Units SCADA MW (XLSX Example)\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# Configure widgets for testing the \"pluto_pc__units_scadamw\" dataset using XLSX files.\n",
                "# These widgets allow dynamic parameterization for file type, storage accounts, sheet name, and key columns.\n",
                "\n",
                "# Step 1: Clear all existing widgets to ensure no duplication or conflicts\n",
                "try:\n",
                "    dbutils.widgets.removeAll()\n",
                "    logger.log_message(\"Existing widgets cleared successfully.\")\n",
                "except Exception as e:\n",
                "    logger.log_error(f\"Error clearing widgets: {str(e)}\")\n",
                "    raise RuntimeError(f\"Failed to initialize widgets: {str(e)}\")\n",
                "\n",
                "# Step 2: Initialize File Type dropdown with supported options\n",
                "dbutils.widgets.dropdown(\"FileType\", \"xlsx\", [\"json\", \"xlsx\", \"xml\"], \"File Type\")\n",
                "\n",
                "# Step 3: Define Source and Destination Storage Accounts\n",
                "dbutils.widgets.text(\"SourceStorageAccount\", \"dplandingstoragetest\", \"Source Storage Account\")\n",
                "dbutils.widgets.text(\"DestinationStorageAccount\", \"dpuniformstoragetest\", \"Destination Storage Account\")\n",
                "\n",
                "# Step 4: Configure Source Container and Dataset Identifier\n",
                "dbutils.widgets.text(\"SourceContainer\", \"landing\", \"Source Container\")\n",
                "dbutils.widgets.text(\"SourceDatasetidentifier\", \"pluto_pc__units_scadamw\", \"Source Datasetidentifier\")\n",
                "\n",
                "# Step 5: Specify Source File Name and Key Columns\n",
                "dbutils.widgets.text(\"SourceFileName\", \"UnitsSCADAMW.xlsx\", \"Source File Name\")\n",
                "dbutils.widgets.text(\"KeyColumns\", \"Unit_GSRN\", \"Key Columns\")\n",
                "\n",
                "# Step 6: Specify Additional Parameter for Sheet Name\n",
                "dbutils.widgets.text(\"SheetName\", \"Sheet\", \"Sheet Name\")\n",
                "\n",
                "# Log a summary of widget initialization\n",
                "logger.log_block(\"Widget Initialization Summary\", [\n",
                "    \"FileType: xlsx (default)\",\n",
                "    \"SourceStorageAccount: dplandingstoragetest\",\n",
                "    \"DestinationStorageAccount: dpuniformstoragetest\",\n",
                "    \"SourceContainer: landing\",\n",
                "    \"SourceDatasetidentifier: pluto_pc__units_scadamw\",\n",
                "    \"SourceFileName: UnitsSCADAMW.xlsx\",\n",
                "    \"KeyColumns: Unit_GSRN\",\n",
                "    \"SheetName: Sheet\"\n",
                "])"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "collapsed": true,
                    "inputWidgets": {},
                    "nuid": "0b2c6046-852d-4895-b393-d11996160c25",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "d0bace8b-8f25-4e1c-bb13-548e97c20417"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Setup"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "b01f8a5c-7c4b-49cd-9858-9eeb0f2ab315",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "f5f1356e-0335-4ecc-85fa-4deba4eff327"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Package Installation and Management"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "53b26632-2c7e-4b39-9265-bf63f2a96697",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "1a2b8219-2e88-4a51-9a54-98718fceac03"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Setup: Package Installation and Management\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# Manage and install essential Python packages for the Databricks project.\n",
                "# Ensures compatibility by specifying exact package versions where necessary.\n",
                "# Includes support for utilities, data processing, and XML/XLSX handling.\n",
                "\n",
                "# Step 1: Optional - Remove an existing version of the custom utility package\n",
                "# Uncomment the line below if a previous version of the utility needs to be removed.\n",
                "# %pip uninstall databricks-custom-utils -y\n",
                "\n",
                "# Step 2: Install required packages\n",
                "# The command below installs:\n",
                "# - Custom Databricks utilities (specific version from GitHub repository).\n",
                "# - Libraries for SQL parsing, Excel file handling, XML processing, and syntax highlighting.\n",
                "%pip install \\\n",
                "    git+https://github.com/Open-Dataplatform/utils-databricks.git@v0.7.1 \\\n",
                "    sqlparse \\\n",
                "    openpyxl \\\n",
                "    lxml \\\n",
                "    xmlschema \\\n",
                "    pygments\n",
                "\n",
                "\"\"\"\n",
                "Package Details:\n",
                "- `utils-databricks`: Custom utilities for extended functionality in Databricks.\n",
                "- `sqlparse`: SQL query parsing and formatting library.\n",
                "- `openpyxl`: Library for handling Excel (XLSX) files.\n",
                "- `lxml`: Robust library for processing XML and HTML files.\n",
                "- `xmlschema`: Tools for XML schema validation and conversion.\n",
                "- `pygments`: Syntax highlighting for code snippets in logs or reports.\n",
                "\"\"\""
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "bc867dd2-532b-4e37-bb76-be96acf95054",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "d8c7b4bf-8c75-4dea-a532-787be944e060"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Initialize Logger"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "88ef1a0d-c92f-4555-bf68-1b51a4b2a666",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "494d8e96-59fe-4047-a809-0e777644684f"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Initialize Logger\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# Set up a custom logger for detailed logging and debugging throughout the notebook.\n",
                "# The logger offers advanced features, including:\n",
                "# - Debug-level logging for in-depth insights during execution.\n",
                "# - Block-style logging for structured, readable logs.\n",
                "# - Syntax highlighting for SQL queries and Python code in logs.\n",
                "\n",
                "# Step 1: Import the Logger class from the custom utilities package\n",
                "from custom_utils.logging.logger import Logger\n",
                "\n",
                "# Step 2: Initialize the Logger instance\n",
                "# - `debug=True` enables detailed logs, useful for troubleshooting and analysis.\n",
                "logger = Logger(debug=True)\n",
                "\n",
                "# Log the initialization success\n",
                "logger.log_message(\"Logger initialized successfully.\")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "96254bc3-2ad1-497d-bbd8-0fb9caa5a8a6",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "49e1195f-a1c4-424f-b7e6-4597df962a33"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Initialize Notebook and Retrieve Parameters"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "c176ea42-b7e2-4452-a863-6b21856c7d58",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "cba10322-164f-4dca-a525-df478ffe9bb3"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Initialize Notebook and Retrieve Parameters\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# Set up the notebook by initializing its configuration and retrieving essential parameters.\n",
                "# This ensures centralized management of settings and enables efficient debugging\n",
                "# through a consistent configuration framework.\n",
                "\n",
                "# Step 1: Import the Config class from the custom utilities package\n",
                "from custom_utils.config.config import Config\n",
                "\n",
                "# Step 2: Initialize the Config object\n",
                "# - Pass `dbutils` for accessing Databricks workspace resources.\n",
                "# - Set `debug=False` to disable verbose debug logs for cleaner execution.\n",
                "config = Config.initialize(dbutils=dbutils, debug=False)\n",
                "\n",
                "# Step 3: Unpack configuration parameters\n",
                "# - Extracts configuration values into the notebook's global scope.\n",
                "# - This simplifies access to parameters by making them available as standard variables.\n",
                "config.unpack(globals())"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "d66fa72a-2159-4252-b4df-b7c68c668fe5",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "56c9e94c-5c0c-4598-81df-dfaa5e6eae99"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Verify paths and files"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "d3f7c883-d519-44cb-ba64-b4c6ea08eb75",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "99a71035-f937-4651-a87a-aeb399d7c1d7"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Verify Paths and Files\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# Validate the required paths and files to ensure all necessary resources \n",
                "# are available for processing. This pre-check prevents runtime errors \n",
                "# by identifying and addressing issues early in the notebook execution.\n",
                "\n",
                "# Step 1: Import the Validator class from the custom utilities package\n",
                "from custom_utils.validation.validation import Validator\n",
                "\n",
                "# Step 2: Initialize the Validator\n",
                "# - Pass `config` to access path and file parameters from the configuration.\n",
                "# - Set `debug=False` for standard validation logging without verbose output.\n",
                "validator = Validator(config=config, debug=False)\n",
                "\n",
                "# Step 3: Unpack validation parameters\n",
                "# - Extracts validation-related parameters into the notebook's global scope.\n",
                "validator.unpack(globals())\n",
                "\n",
                "# Step 4: Perform validation and check for an exit flag\n",
                "# - If critical validation fails, the notebook execution is terminated.\n",
                "validator.check_and_exit()\n"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "f09f5cf1-ba07-4fcc-9408-16dcb9aa0095",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "0422b31c-d482-4b28-b15a-f7540c90767c"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Exit the Notebook if Validation Fails"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "ea7667b5-ff45-402f-b89a-fa071c62c090",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "36b253cc-8f27-4f48-961c-68971940a9ca"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Exit the Notebook if Validation Fails\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# Stop notebook execution gracefully if critical validation checks fail.\n",
                "# If validation passes, continue processing with a confirmation message.\n",
                "\n",
                "# Step 1: Check for an exit condition flagged by the Validator\n",
                "if Validator.exit_notebook:\n",
                "    # Step 2: Log the exit message using the logger\n",
                "    # - Provides context on why the notebook execution is being terminated.\n",
                "    logger.log_error(Validator.exit_notebook_message, level=\"error\")\n",
                "    \n",
                "    # Step 3: Exit the notebook with a descriptive message\n",
                "    # - Uses Databricks utilities to terminate execution cleanly.\n",
                "    dbutils.notebook.exit(f\"Notebook exited: {Validator.exit_notebook_message}\")\n",
                "else:\n",
                "    # Step 4: Log a success message if validation passed\n",
                "    # - Confirms the notebook will continue execution.\n",
                "    logger.log_message(\"Validation passed. The notebook is proceeding without exiting.\", level=\"info\")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "9245ce8b-8064-4392-961e-87d13a3dbea4",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "200d5de0-5156-433b-aee2-f6715efdc131"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Processing Workflow"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "ce29e7ed-b443-4187-95cc-40020ece5088",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "54c982d3-37ff-41ae-aedc-77dfbfd076ca"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Flattening and Processing"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "20671e6a-d5d5-4d87-95fb-3097241c2c4c",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "224e882c-fb36-46a8-ac32-4e378fecae42"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Processing Workflow - Flattening and Processing\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# This section executes the core data processing workflow, which includes:\n",
                "# - Flattening complex hierarchical data for simplified querying and analysis.\n",
                "# - Applying dataset-specific transformations to align with business requirements.\n",
                "\n",
                "from pyspark.sql.functions import col\n",
                "from custom_utils.transformations.dataframe import DataFrameTransformer\n",
                "\n",
                "# Initialize the DataFrameTransformer\n",
                "# - Uses the current configuration and disables debug mode for standard operation.\n",
                "transformer = DataFrameTransformer(config=config, debug=False)\n",
                "\n",
                "try:\n",
                "    # Step 1: Process and flatten the data\n",
                "    # - Produces both the initial DataFrame and its flattened version.\n",
                "    df_initial, df_flattened = transformer.process_and_flatten_data(depth_level=depth_level)\n",
                "\n",
                "    # Step 2: Apply dataset-specific transformations (if applicable)\n",
                "    # Triton flow plans: Rename and cast the \"Timestamp\" column to \"EventTimestamp\"\n",
                "    if config.source_datasetidentifier == \"triton__flow_plans\":\n",
                "        df_flattened = (\n",
                "            df_flattened\n",
                "            .withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
                "            .withColumnRenamed(\"Timestamp\", \"EventTimestamp\")\n",
                "        )\n",
                "        logger.log_message(\"Applied transformations for 'triton__flow_plans'.\", level=\"info\")\n",
                "\n",
                "    # CPX SO nomination: Cast relevant fields to timestamp for consistency\n",
                "    if config.source_datasetidentifier == \"cpx_so__nomination\":\n",
                "        df_flattened = (\n",
                "            df_flattened\n",
                "            .withColumn(\"dateCreated\", col(\"dateCreated\").cast(\"timestamp\"))\n",
                "            .withColumn(\"validityPeriod_begin\", col(\"validityPeriod_begin\").cast(\"timestamp\"))\n",
                "            .withColumn(\"validityPeriod_end\", col(\"validityPeriod_end\").cast(\"timestamp\"))\n",
                "            .withColumn(\"flows_periods_validityPeriod_begin\", col(\"flows_periods_validityPeriod_begin\").cast(\"timestamp\"))\n",
                "            .withColumn(\"flows_periods_validityPeriod_end\", col(\"flows_periods_validityPeriod_end\").cast(\"timestamp\"))\n",
                "        )\n",
                "        logger.log_message(\"Applied transformations for 'cpx_so__nomination'.\", level=\"info\")\n",
                "\n",
                "    # Step 3: Display the initial and flattened DataFrames for user verification\n",
                "    logger.log_block(\"Displaying the initial and flattened DataFrames.\", level=\"info\")\n",
                "    logger.log_message(\"Initial DataFrame:\", level=\"info\")\n",
                "    display(df_initial)\n",
                "\n",
                "    logger.log_message(\"Flattened DataFrame:\", level=\"info\")\n",
                "    display(df_flattened)\n",
                "\n",
                "except Exception as e:\n",
                "    # Step 4: Handle errors gracefully\n",
                "    # - Logs the error details for debugging and terminates the process.\n",
                "    logger.log_message(f\"Error during processing: {str(e)}\", level=\"error\")\n",
                "    dbutils.notebook.exit(f\"Processing failed: {str(e)}\")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "baa8fc55-134f-4ba1-8290-07a3eea5fbc4",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "27361ed4-2443-4d66-b0b2-a41b58900aea"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Quality check "
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "085b86e5-b48d-498b-8a05-6416832c293a",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "0898e084-c8f4-47d8-897e-7faf83a56bc1"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Perform Quality Check and Remove Duplicates"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "f4d08f7e-8289-4b54-a81c-eaa96a0cb0be",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "a7ff6293-acaf-494d-8a49-3a96c9d7de01"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Quality Check - Perform Quality Check and Remove Duplicates\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# This section performs data quality checks to ensure:\n",
                "# - The integrity, accuracy, and consistency of the processed data.\n",
                "# - Duplicate records are identified and optionally removed.\n",
                "# - Additional quality checks (e.g., null value checks, value range checks) are executed.\n",
                "\n",
                "from custom_utils.quality.quality import DataQualityManager\n",
                "\n",
                "# Step 1: Initialize the DataQualityManager\n",
                "quality_manager = DataQualityManager(logger=logger, debug=True)\n",
                "\n",
                "# Step 2: Log available quality checks\n",
                "# This provides an overview of checks supported by the quality manager.\n",
                "quality_manager.describe_available_checks()\n",
                "\n",
                "# Step 3: Execute data quality checks on the flattened DataFrame\n",
                "try:\n",
                "    # Perform quality checks with the following configurations:\n",
                "    # - Partitioning and duplicate checks are based on `key_columns`.\n",
                "    # - Duplicate removal uses `feedback_column` for ordering within partitions.\n",
                "    # - Exclude the `input_file_name` column from the final processed DataFrame.\n",
                "    cleaned_data_view = quality_manager.perform_data_quality_checks(\n",
                "        spark=spark,\n",
                "        df=df_flattened,\n",
                "        key_columns=key_columns,  # Key columns for partitioning and duplicate checking\n",
                "        order_by=feedback_column,  # Columns for ordering within partitions\n",
                "        feedback_column=feedback_column,  # Column for duplicate removal ordering\n",
                "        join_column=key_columns,  # Column for referential integrity check\n",
                "        columns_to_exclude=[\"input_file_name\"],  # Columns to exclude from final DataFrame\n",
                "        use_python=False  # Use SQL-based quality checks\n",
                "    )\n",
                "\n",
                "except Exception as e:\n",
                "    # Handle any errors during the quality check process\n",
                "    logger.log_error(f\"Error during quality check: {str(e)}\")\n",
                "    raise RuntimeError(f\"Quality check failed: {str(e)}\")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "ffec0b21-8b5f-43e1-8d4a-6ff41a9ae3e5",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "b38c1f89-5ad4-4b91-93fe-fab7507b5d8e"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Unified Data Management"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "eabbf4d3-4ec1-4906-b613-ca3bb7b0a30d",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "b917ed53-7220-403b-9624-9f1071dc39e1"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Table Creation and Data Merging"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "ef3d2f57-0b96-4ac3-b4de-c80df0a9f996",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "4d34f1bd-d345-4bd0-bc0e-fb2ae3cba7a1"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Unified Data Management: Table Creation and Data Merging\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# This section handles the creation of destination tables and merges\n",
                "# processed data into the respective storage location. It ensures:\n",
                "# - Data is written to a unified storage with consistent formatting.\n",
                "# - Merging supports updates, inserts, and deletions seamlessly.\n",
                "# - Storage operations are managed efficiently with robust logging.\n",
                "\n",
                "from custom_utils.catalog.catalog_utils import DataStorageManager\n",
                "\n",
                "# Step 1: Initialize the DataStorageManager\n",
                "storage_manager = DataStorageManager(logger=logger, debug=True)\n",
                "\n",
                "# Step 2: Perform the data storage operation\n",
                "try:\n",
                "    # Manage data operation to handle:\n",
                "    # - Writing data to a destination folder.\n",
                "    # - Creating a table if it does not exist.\n",
                "    # - Merging processed data into the existing table.\n",
                "    storage_manager.manage_data_operation(\n",
                "        spark=spark,\n",
                "        dbutils=dbutils,\n",
                "        cleaned_data_view=cleaned_data_view,  # The view containing cleaned and transformed data\n",
                "        key_columns=key_columns,  # Columns used to match and merge records\n",
                "        destination_folder_path=destination_data_folder_path,  # Path for storing Delta files\n",
                "        destination_environment=destination_environment,  # Database name\n",
                "        source_datasetidentifier=source_datasetidentifier,  # Target table name\n",
                "        use_python=False  # False indicates SQL-based operations\n",
                "    )\n",
                "\n",
                "    # Log success\n",
                "    logger.log_message(\"Data successfully written and merged into the destination table.\")\n",
                "\n",
                "except Exception as e:\n",
                "    # Handle any errors during the data storage process\n",
                "    logger.log_error(f\"Error during data storage operation: {str(e)}\")\n",
                "    raise RuntimeError(f\"Data storage operation failed: {str(e)}\")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "85023e47-eda5-4261-8b37-c10aef3c5b69",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "6bfaeb5f-cb1a-4be4-9d0a-3ef0c3a078a3"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Finishing"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "d6e89aa8-7435-4f5e-84b6-401f59960dfc",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "df0fa067-31e4-46b4-93c7-d831dd794ab0"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Return period (from_datetime, to_datetime) covered by data read"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "6b2dc900-70de-42a7-8fae-6cfb150011a6",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "0324318a-bf41-44e5-a32f-c1905b233d24"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Finishing - Return Period Covered by Data Read\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# This section generates the feedback timestamps, providing the time \n",
                "# period covered by the processed and stored data. It extracts the \n",
                "# `from_datetime` and `to_datetime` based on the cleaned data view.\n",
                "\n",
                "# Step 1: Generate feedback timestamps\n",
                "try:\n",
                "    notebook_output = storage_manager.generate_feedback_timestamps(\n",
                "        spark=spark,\n",
                "        view_name=cleaned_data_view,  # The view containing cleaned and processed data\n",
                "        feedback_column=feedback_column,  # Column used for identifying feedback periods\n",
                "        key_columns=key_columns  # Key columns for grouping and extracting timestamp bounds\n",
                "    )\n",
                "\n",
                "except Exception as e:\n",
                "    # Handle errors during feedback timestamp generation\n",
                "    logger.log_error(f\"Error generating feedback timestamps: {str(e)}\")\n",
                "    raise RuntimeError(f\"Failed to generate feedback timestamps: {str(e)}\")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "85f6e198-9eb1-473a-b7b3-89d78516f86e",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "81e75790-f0f1-434c-90b9-6a83c0dab003"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Exit the notebook"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "6cd18333-5e15-45ca-8105-dd6823a9a3cc",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "c0a20a77-38d7-4ca4-a372-54520c25fef6"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ==============================================================\n",
                "# Exit the Notebook\n",
                "# ==============================================================\n",
                "\n",
                "# Purpose:\n",
                "# Conclude the notebook execution and return the output summarizing \n",
                "# the period covered by the processed data.\n",
                "\n",
                "# Step 1: Exit the notebook with the output\n",
                "try:\n",
                "    dbutils.notebook.exit(notebook_output)\n",
                "    logger.log_message(f\"Notebook exited successfully with output: {notebook_output}\")\n",
                "\n",
                "except Exception as e:\n",
                "    logger.log_error(f\"Error during notebook exit: {str(e)}\")\n",
                "    raise RuntimeError(f\"Failed to exit the notebook: {str(e)}\")"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "029a6b4e-8e74-43bd-b67c-d0b7a3109808",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                },
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                },
                "azdata_cell_guid": "e48765da-bc8d-4a92-90eb-f4050750b1b7"
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}