{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/Open-Dataplatform/utils-databricks.git@v0.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from custom_utils.dp_storage import connector, reader, writer\n",
    "from custom_utils import adf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7522a797",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8041c2d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e523946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mount_point = connector.mount(dbutils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f0f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.widgets.removeAll()\n",
    "adf.initialize_config_widgets(dbutils)  # Included to make it easier to add the configs when setting up the notebook.\n",
    "\n",
    "# Get the parameters from ADF. When developing, insert values in the text widgets.\n",
    "source_config = adf.get_source_config(dbutils)\n",
    "destination_config = adf.get_destination_config(dbutils)\n",
    "\n",
    "source_folder_path = adf.get_parameter(dbutils, 'SourceFolderPath')  # Remember that it has the format \"<container>/<directory>\"\n",
    "source_filename = adf.get_parameter(dbutils, 'SourceFileName')\n",
    "\n",
    "# TODO: If you have want more parameters from ADF, add them here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1422d9b",
   "metadata": {},
   "source": [
    "## Read\n",
    "Reads data from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdb501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file_path = reader.get_path_to_triggering_file(\n",
    "    mount_point,\n",
    "    source_folder_path,\n",
    "    source_filename,\n",
    "    config_for_triggered_dataset=source_config['TODO: dataset_identifier(not guid)']\n",
    ")\n",
    "\n",
    "df = spark.read.parquet(source_file_path)\n",
    "# Rewrite the line above, if your file is not parquet. Example for csv:\n",
    "# df = spark.read.option(\"delimiter\", \",\").csv(source_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5156013",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b159ffa9",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0fa14e",
   "metadata": {},
   "source": [
    "Transform the data here. Follow this style guide: https://github.com/palantir/pyspark-style-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e922f783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edc6b0b6",
   "metadata": {},
   "source": [
    "## Merge and upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087cfdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update the following parameters\n",
    "timestamp_column = 'NAME_OF_TIMESTAMP_COLUMN'\n",
    "index_columns = [timestamp_column, 'ANOTHER_KEY_COLUMN']\n",
    "time_resolution_egress = 'month'  # 'hour', 'month', 'day', or 'hour'\n",
    "egress_identifier = destination_config['YOUR_EGRESS_DATASET_IDENTIFIER(not guid)']['dataset']\n",
    "\n",
    "# Merge data into the existing egress data.\n",
    "# If the timestamp column is a string column, add the format in the key word parameter time_format.\n",
    "# (example: time_format=\"yyyy-MM-dd'T'HH:mm:ss'Z'\")\n",
    "writer.merge_and_upload(\n",
    "    df,\n",
    "    egress_identifier,\n",
    "    timestamp_column,\n",
    "    index_columns,\n",
    "    time_resolution_egress,\n",
    "    mount_point,\n",
    "    spark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4968fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always keep this at the end of your notebook\n",
    "connector.unmount_if_prod(mount_point, dbutils)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
